{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "import glob\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_txt_to_corpus(path):\n",
    "    corpus = []\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            corpus.append(l.split('\\n')[0].split(' '))\n",
    "        f.close()\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_root = './experiment-data'\n",
    "model_types = ['base', 'as_is']\n",
    "N_lists = ['6']\n",
    "trials = 5\n",
    "scores = {}\n",
    "paths = glob.glob(f'{files_root}/*')\n",
    "\n",
    "for mt in model_types:\n",
    "    scores[mt] = {}\n",
    "    for N in N_lists:\n",
    "        if mt != 'base' and N == '1':\n",
    "            continue\n",
    "        total_bleu = 0\n",
    "        for t in range(trials):\n",
    "            perds_path = None\n",
    "            refs_path = None\n",
    "            keywords = [mt, f'N={N}', f'trial={t}']\n",
    "            for p in paths:\n",
    "                is_match = True\n",
    "                for kw in keywords:\n",
    "                    is_match = is_match and (kw in p)\n",
    "                if is_match:\n",
    "                    if '_preds' in p:\n",
    "                        perds_path = p\n",
    "                    elif '_refs' in p:\n",
    "                        refs_path = p\n",
    "            candidate_corpus = turn_txt_to_corpus(perds_path)\n",
    "            print(candidate_corpus[:2])\n",
    "            references_corpus = turn_txt_to_corpus(refs_path)\n",
    "            total_bleu = total_bleu + bleu_score(candidate_corpus, references_corpus)\n",
    "            print(mt, N, t, 'done')\n",
    "        scores[mt][N] = total_bleu / trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torchmetrics\n",
    "except:\n",
    "    !pip install torchmetrics\n",
    "\n",
    "try:\n",
    "    import evaluate\n",
    "except:\n",
    "    !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(fn, data):\n",
    "    with open(fn) as file:\n",
    "        while line := file.readline():\n",
    "            data.append(line.rstrip())\n",
    "            # break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=0_preds.txt\n",
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=0_src.txt\n",
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=0_refs.txt\n",
      "Adding surrounding list for each target\n",
      "Adding surrounding list for each target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=1_refs.txt\n",
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=1_preds.txt\n",
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=1_src.txt\n",
      "Adding surrounding list for each target\n",
      "Adding surrounding list for each target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=2_src.txt\n",
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=2_refs.txt\n",
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=2_preds.txt\n",
      "Adding surrounding list for each target\n",
      "Adding surrounding list for each target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=3_src.txt\n",
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=3_refs.txt\n",
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=3_preds.txt\n",
      "Adding surrounding list for each target\n",
      "Adding surrounding list for each target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=4_refs.txt\n",
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=4_src.txt\n",
      "./experiment-data/model=base_N=6_lang_pair=de->en_trial=4_preds.txt\n",
      "Adding surrounding list for each target\n",
      "Adding surrounding list for each target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=0_preds.txt\n",
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=0_src.txt\n",
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=0_refs.txt\n",
      "Adding surrounding list for each target\n",
      "Adding surrounding list for each target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=1_preds.txt\n",
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=1_refs.txt\n",
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=1_src.txt\n",
      "Adding surrounding list for each target\n",
      "Adding surrounding list for each target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=2_preds.txt\n",
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=2_src.txt\n",
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=2_refs.txt\n",
      "Adding surrounding list for each target\n",
      "Adding surrounding list for each target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=3_refs.txt\n",
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=3_preds.txt\n",
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=3_src.txt\n",
      "Adding surrounding list for each target\n",
      "Adding surrounding list for each target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=4_src.txt\n",
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=4_preds.txt\n",
      "./experiment-data/model=as_is_N=6_lang_pair=de->en_trial=4_refs.txt\n",
      "Adding surrounding list for each target\n",
      "Adding surrounding list for each target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_de-&gt;en_6</th>\n",
       "      <th>as_is_de-&gt;en_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BLEU</th>\n",
       "      <td>(0.29382062, 0.0027804063)</td>\n",
       "      <td>(0.2925221, 0.0026590193)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SacreBLEU</th>\n",
       "      <td>(0.29382062, 0.0027804063)</td>\n",
       "      <td>(0.2925221, 0.0026590193)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>METEOR</th>\n",
       "      <td>(0.57, 0.0)</td>\n",
       "      <td>(0.5679999999999998, 0.0039999999999999584)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMET</th>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(0, 0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        base_de->en_6  \\\n",
       "BLEU       (0.29382062, 0.0027804063)   \n",
       "SacreBLEU  (0.29382062, 0.0027804063)   \n",
       "METEOR                    (0.57, 0.0)   \n",
       "COMET                          (0, 0)   \n",
       "\n",
       "                                        as_is_de->en_6  \n",
       "BLEU                         (0.2925221, 0.0026590193)  \n",
       "SacreBLEU                    (0.2925221, 0.0026590193)  \n",
       "METEOR     (0.5679999999999998, 0.0039999999999999584)  \n",
       "COMET                                           (0, 0)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics import BLEUScore, SacreBLEUScore\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model_names = ['base', 'as_is']\n",
    "# language_pairings = [('de', 'en'), ('en', 'de')]\n",
    "language_pairings = [('de', 'en')]\n",
    "num_trials = 5\n",
    "N_lists = ['6']\n",
    "path_root = './experiment-data'\n",
    "paths = glob.glob(f'{path_root}/*')\n",
    "\n",
    "df_dict = {}\n",
    "for mn in model_names:\n",
    "    for lp in language_pairings:\n",
    "        for N in N_lists:\n",
    "            if mn != 'base' and N =='1':\n",
    "                continue\n",
    "            lp_str = f'{lp[0]}->{lp[1]}'\n",
    "            df_dict[f'{mn}_{lp_str}_{N}'] = {\n",
    "                'BLEU' : (0,0), # format key as tuple (mean, std)\n",
    "                'SacreBLEU' : (0,0),\n",
    "                'METEOR' : (0,0),\n",
    "                'COMET' : (0,0)\n",
    "            }\n",
    "            metrics_dict = {\n",
    "                'bleu' : [],\n",
    "                'sacrebleu' : [],\n",
    "                'meteor' : [],\n",
    "                'comet' : []\n",
    "            }\n",
    "            for i in range(num_trials):\n",
    "                \n",
    "                p_ct = 0\n",
    "                for p in paths:\n",
    "                    if mn in p and f'trial={i}' in p and f'N={N}' in p:\n",
    "                        if 'preds' in p:\n",
    "                            preds_path = p\n",
    "                        elif 'refs' in p:\n",
    "                            refs_path = p\n",
    "                        elif 'src' in p:\n",
    "                            src_path = p\n",
    "                        p_ct += 1\n",
    "                        print(p)\n",
    "                assert p_ct == 3\n",
    "\n",
    "                # read data into lists\n",
    "                preds = []\n",
    "                refs = []\n",
    "                src = []\n",
    "                readfile(preds_path, preds)\n",
    "                readfile(refs_path, refs)\n",
    "                readfile(src_path, src)\n",
    "\n",
    "                # bleu\n",
    "                if len(refs[0]) > 1:\n",
    "                    print('Adding surrounding list for each target')\n",
    "                    refs_bleu = [[x] for x in refs]\n",
    "                else:\n",
    "                    refs_bleu = refs\n",
    "\n",
    "                bleu = BLEUScore()\n",
    "                torch_bleu = bleu(preds, refs_bleu)\n",
    "                metrics_dict['bleu'].append(torch_bleu)\n",
    "\n",
    "                # sacrebleu\n",
    "                if len(refs[0]) > 1:\n",
    "                    print('Adding surrounding list for each target')\n",
    "                    refs_sacrebleu = [[x] for x in refs]\n",
    "                else:\n",
    "                    refs_sacrebleu = refs\n",
    "\n",
    "                sacre_bleu = SacreBLEUScore(tokenize='none')\n",
    "                torch_sacrebleu = sacre_bleu(preds, refs_sacrebleu)\n",
    "                metrics_dict['sacrebleu'].append(torch_sacrebleu)\n",
    "\n",
    "                # meteor\n",
    "                meteor = evaluate.load('meteor')\n",
    "                results = meteor.compute(predictions=preds, references=refs)\n",
    "                huggingface_meteor = (round(results['meteor'], 2))\n",
    "                metrics_dict['meteor'].append(huggingface_meteor)\n",
    "\n",
    "            # compute mean and std over trials and put in dict\n",
    "            mean_bleu = np.mean(np.array(metrics_dict['bleu']))\n",
    "            std_bleu = np.std(np.array(metrics_dict['bleu']))\n",
    "\n",
    "            mean_sacrebleu = np.mean(np.array(metrics_dict['sacrebleu']))\n",
    "            std_sacrebleu = np.std(np.array(metrics_dict['sacrebleu']))\n",
    "\n",
    "            mean_meteor = np.mean(np.array(metrics_dict['meteor']))\n",
    "            std_meteor = np.std(np.array(metrics_dict['meteor']))\n",
    "\n",
    "            # mean_comet = np.mean(np.array(metrics_dict['comet']))\n",
    "            # std_comet = np.std(np.array(metrics_dict['comet']))\n",
    "\n",
    "\n",
    "            df_dict[f'{mn}_{lp_str}_{N}'] = {\n",
    "                'BLEU' : (mean_bleu,std_bleu), # format key as tuple (mean, std)\n",
    "                'SacreBLEU' : (mean_sacrebleu,std_sacrebleu),\n",
    "                'METEOR' : (mean_meteor,std_meteor),\n",
    "                'COMET' : (0,0) # disabled for now\n",
    "            }\n",
    "\n",
    "results_df = pd.DataFrame(df_dict)\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
