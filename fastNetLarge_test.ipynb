{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyle/pythonenvs/venv38/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from data.data import get_train_test_loader\n",
    "from model.network import FastUpdateNet, FastUpdateNetLarge, FastUpdateNetLarge_Better, FastUpdateNetLarge_Best\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_train_test_loader('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import multiprocessing as mp\n",
    "\n",
    "def compute_back_M1(network,Q):\n",
    "  network.mNet1.backwardHidden()\n",
    "\n",
    "def compute_back_M2(network):\n",
    "  network.mNet2.backwardHidden()\n",
    "\n",
    "def train(epoch, network):\n",
    "  mp.set_start_method('spawn', force=True)\n",
    "  network.train()\n",
    "  network.share_memory()\n",
    "  manager = mp.Manager()\n",
    "    # q = manager.Queue()\n",
    "  Q = manager.Queue()\n",
    "\n",
    "\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data= data.to('cuda:1')\n",
    "    target = target.to('cuda:1')\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    print(network.mNet1.saver.g.clone().detach())\n",
    "    # Q.put(network.mNet1.saver.g.clone().detach())\n",
    "    # z = (Q.get())\n",
    "    # print(z)\n",
    "    # print(network.mNet.saver.grad)\n",
    "    # p1 = Thread(target=compute_back_M1, args=[network]) # start two independent threads\n",
    "    # p2 = Thread(target=compute_back_M2, args=[network])\n",
    "    \n",
    "    p1 = mp.Process(target=compute_back_M1, args=(network,Q,))\n",
    "    # p2 = mp.Process(target=compute_back_M2, args=(network,))\n",
    "\n",
    "    p1.start()\n",
    "    Q.put(network.mNet1.saver.g.clone().detach())\n",
    "    # p2.start()\n",
    "    # network.mNet1.backwardHidden()\n",
    "    # network.mNet2.backwardHidden()\n",
    "    p1.join() # wait for the two threads to finish\n",
    "    # return \n",
    "    # p2.join()\n",
    "\n",
    "    correctness = (target == torch.argmax(output))\n",
    "    optimizer.step()\n",
    "    # network.mNet.weightUpdate(correctness, lr = learning_rate)\n",
    "    if batch_idx % log_interval == 0:\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(network):\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      data = data.to('cuda:1')\n",
    "      target = target.to('cuda:1')\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies.append(100. * correct / len(test_loader.dataset))\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test using 2 Ms (computing grad in main thread and separate thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "fNet = FastUpdateNetLarge_Best().to('cuda:1')\n",
    "# fNet.mNet1 = torch.nn.Sequential(nn.Linear(392, 196), nn.ReLU(), nn.Linear(196, 98), nn.ReLU(), nn.Linear(98, 49), nn.ReLU())\n",
    "# fNet.mNet2 = torch.nn.Sequential(nn.Linear(392, 196), nn.ReLU(), nn.Linear(196, 98), nn.ReLU(), nn.Linear(98, 49), nn.ReLU())\n",
    "optimizer = optim.Adam(fNet.parameters(), lr=learning_rate,)# momentum=momentum)\n",
    "\n",
    "# optimizer = optim.SGD(fNet.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3034, Accuracy: 1137/10000 (11%)\n",
      "\n",
      "tensor([[-1.1372e-07,  1.7499e-06,  4.0468e-06,  ..., -1.5531e-06,\n",
      "          1.5421e-06,  2.4397e-06],\n",
      "        [-7.2685e-07, -1.7909e-07,  1.9557e-06,  ...,  2.7634e-07,\n",
      "          2.8329e-06,  2.2409e-06],\n",
      "        [ 2.1226e-06,  1.8723e-06,  2.0437e-06,  ...,  4.6153e-07,\n",
      "          6.1646e-07,  1.8275e-06],\n",
      "        ...,\n",
      "        [ 3.0587e-06,  2.6507e-07,  2.5113e-06,  ..., -5.7597e-07,\n",
      "          3.9536e-06,  3.1102e-06],\n",
      "        [ 5.8064e-06, -7.6511e-06, -3.5439e-06,  ...,  4.3266e-06,\n",
      "         -6.8537e-06,  2.5456e-07],\n",
      "        [-1.1178e-05,  6.4735e-06,  3.9543e-06,  ..., -4.8199e-06,\n",
      "          1.4202e-06, -5.3931e-06]], device='cuda:1')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m test(fNet)\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m   train(epoch, fNet)\n\u001b[1;32m     12\u001b[0m   test(fNet)\n\u001b[1;32m     13\u001b[0m   \u001b[39m# if (epoch + 1) % 10 == 0:\u001b[39;00m\n\u001b[1;32m     14\u001b[0m   \u001b[39m#   torch.save(fNet, 'fNet-stdp-30000-epoch.pt')\u001b[39;00m\n\u001b[1;32m     15\u001b[0m   \u001b[39m#   with open('fNet-stdp-3000-epoch-loss-accuracy.pkl', 'wb') as f:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m   \u001b[39m#     pickle.dump({'train_losses':train_losses, 'test_losses': test_losses, 'test_accuracies':test_accuracies}, f, protocol=pickle.HIGHEST_PROTOCOL)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[39m# torch.save(fNet, 'fNet-stdp-30000-epoch.pt')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, network)\u001b[0m\n\u001b[1;32m     35\u001b[0m p1 \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mProcess(target\u001b[39m=\u001b[39mcompute_back_M1, args\u001b[39m=\u001b[39m(network,Q,))\n\u001b[1;32m     36\u001b[0m \u001b[39m# p2 = mp.Process(target=compute_back_M2, args=(network,))\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m p1\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m     39\u001b[0m Q\u001b[39m.\u001b[39mput(network\u001b[39m.\u001b[39mmNet1\u001b[39m.\u001b[39msaver\u001b[39m.\u001b[39mg\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m     40\u001b[0m \u001b[39m# p2.start()\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# network.mNet1.backwardHidden()\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# network.mNet2.backwardHidden()\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, fp)\n\u001b[1;32m     48\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "File \u001b[0;32m~/pythonenvs/venv38/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:153\u001b[0m, in \u001b[0;36mreduce_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    150\u001b[0m storage \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mstorage()\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m tensor\u001b[39m.\u001b[39mrequires_grad \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m tensor\u001b[39m.\u001b[39mis_leaf:\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCowardly refusing to serialize non-leaf tensor which requires_grad, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39msince autograd does not support crossing process boundaries.  \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mIf you just want to transfer the data, call detach() on the tensor \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mbefore serializing (e.g., putting it on the queue).\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m check_serializing_named_tensor(tensor)\n\u001b[1;32m    159\u001b[0m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mhooks\u001b[39m.\u001b[39mwarn_if_has_hooks(tensor)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue)."
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "s = time.perf_counter()\n",
    "# print(fNet)\n",
    "\n",
    "test(fNet)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch, fNet)\n",
    "  test(fNet)\n",
    "  # if (epoch + 1) % 10 == 0:\n",
    "  #   torch.save(fNet, 'fNet-stdp-30000-epoch.pt')\n",
    "  #   with open('fNet-stdp-3000-epoch-loss-accuracy.pkl', 'wb') as f:\n",
    "  #     pickle.dump({'train_losses':train_losses, 'test_losses': test_losses, 'test_accuracies':test_accuracies}, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# torch.save(fNet, 'fNet-stdp-30000-epoch.pt')\n",
    "e = time.perf_counter()\n",
    "print(f\"Training time for {epoch} epochs: {e-s} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(13.2600, device='cuda:1'),\n",
       " tensor(89.8500, device='cuda:1'),\n",
       " tensor(91.0900, device='cuda:1'),\n",
       " tensor(79.4900, device='cuda:1'),\n",
       " tensor(86.5800, device='cuda:1'),\n",
       " tensor(88.8400, device='cuda:1')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test using no Ms, and only on main thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_no_M(epoch, network):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data = data.to('cuda:1')\n",
    "    target = target.to('cuda:1')\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    correctness = (target == torch.argmax(output))\n",
    "    optimizer.step()\n",
    "    # network.mNet.weightUpdate(correctness, lr = learning_rate)\n",
    "    if batch_idx % log_interval == 0:\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "fNet = FastUpdateNetLarge_Best(use_M=False).to('cuda:1')\n",
    "\n",
    "optimizer = optim.Adam(fNet.parameters(), lr=learning_rate,)# momentum=momentum)\n",
    "\n",
    "# optimizer = optim.SGD(fNet.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3093, Accuracy: 981/10000 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2258, Accuracy: 9429/10000 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2098, Accuracy: 9488/10000 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1996, Accuracy: 9416/10000 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2540, Accuracy: 9508/10000 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2797, Accuracy: 9524/10000 (95%)\n",
      "\n",
      "Training time for 5 epochs: 51.2263676491566 seconds\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "s = time.perf_counter()\n",
    "\n",
    "test(fNet)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train_no_M(epoch, fNet)\n",
    "  test(fNet)\n",
    "  # if (epoch + 1) % 10 == 0:\n",
    "  #   torch.save(fNet, 'fNet-stdp-30000-epoch.pt')\n",
    "  #   with open('fNet-stdp-3000-epoch-loss-accuracy.pkl', 'wb') as f:\n",
    "  #     pickle.dump({'train_losses':train_losses, 'test_losses': test_losses, 'test_accuracies':test_accuracies}, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# torch.save(fNet, 'fNet-stdp-30000-epoch.pt')\n",
    "e = time.perf_counter()\n",
    "print(f\"Training time for {epoch} epochs: {e-s} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ee5443183715725406fd5246e657c0e511f90699501bc5e8c5d8d2b3c204bfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
