{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from data.data import get_train_test_loader\n",
    "from model.network import FastUpdateNet, TeacherNet\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 8\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_train_test_loader('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "def do(network):\n",
    "  network.mNet.backwardHidden() \n",
    "\n",
    "def train_fastNet(epoch, network):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    # print(network.mNet.saver.grad)\n",
    "    p1 = Thread(target=do, args=[network]) # start two independent threads\n",
    "\n",
    "    p1.start()\n",
    "        \n",
    "    p1.join()\n",
    "    # split into threads\n",
    "    correctness = (target == torch.argmax(output))\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def train_teacherNet(epoch, network):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    # print(network.mNet.saver.grad)\n",
    "    correctness = (target == torch.argmax(output))\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(network):\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies.append(100. * correct / len(test_loader.dataset))\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tNet = torch.load('tNet.pt')\n",
    "# fNet = FastUpdateNet(teacherNet=tNet)\n",
    "# test(fNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNet = FastUpdateNet()\n",
    "optimizer = optim.SGD(fNet.get_parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with inv\n",
    "error tensor(0.0003, grad_fn=<SumBackward0>)\n",
    "0.018805503845214844\n",
    "\n",
    "with pinv\n",
    "error tensor(0.00004, grad_fn=<SumBackward0>)\n",
    "0.02195906639099121\n",
    "\n",
    "0.01661086082458496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model type base trail 0 starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/Documents/magic-m/model/network.py:48: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "/home/hao/miniconda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.3878, Accuracy: 8837/10000 (88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2021, Accuracy: 9394/10000 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1521, Accuracy: 9545/10000 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1350, Accuracy: 9610/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1055, Accuracy: 9677/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0971, Accuracy: 9703/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0977, Accuracy: 9697/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0960, Accuracy: 9705/10000 (97%)\n",
      "\n",
      "model type base trail 1 starts\n",
      "\n",
      "Test set: Avg. loss: 0.3429, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1969, Accuracy: 9434/10000 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1461, Accuracy: 9565/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1184, Accuracy: 9649/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1018, Accuracy: 9701/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0851, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0771, Accuracy: 9754/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0749, Accuracy: 9775/10000 (98%)\n",
      "\n",
      "model type base trail 2 starts\n",
      "\n",
      "Test set: Avg. loss: 0.3302, Accuracy: 9005/10000 (90%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2002, Accuracy: 9402/10000 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1508, Accuracy: 9568/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1213, Accuracy: 9630/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0992, Accuracy: 9686/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0945, Accuracy: 9710/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0855, Accuracy: 9752/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0825, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "model type base trail 3 starts\n",
      "\n",
      "Test set: Avg. loss: 0.3482, Accuracy: 8973/10000 (90%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2129, Accuracy: 9322/10000 (93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1455, Accuracy: 9550/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1236, Accuracy: 9633/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1111, Accuracy: 9676/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0976, Accuracy: 9706/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0861, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0837, Accuracy: 9756/10000 (98%)\n",
      "\n",
      "model type base trail 4 starts\n",
      "\n",
      "Test set: Avg. loss: 0.3480, Accuracy: 8982/10000 (90%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2256, Accuracy: 9329/10000 (93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1518, Accuracy: 9545/10000 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1305, Accuracy: 9592/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1058, Accuracy: 9677/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0894, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0847, Accuracy: 9746/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0842, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "model type M trail 0 starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/Documents/magic-m/model/network.py:132: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(o_4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.5021, Accuracy: 8555/10000 (86%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2503, Accuracy: 9230/10000 (92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1642, Accuracy: 9501/10000 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1371, Accuracy: 9582/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1129, Accuracy: 9644/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1113, Accuracy: 9677/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0950, Accuracy: 9699/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0956, Accuracy: 9709/10000 (97%)\n",
      "\n",
      "model type M trail 1 starts\n",
      "\n",
      "Test set: Avg. loss: 0.4086, Accuracy: 8776/10000 (88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2364, Accuracy: 9283/10000 (93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1678, Accuracy: 9501/10000 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1372, Accuracy: 9589/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1274, Accuracy: 9598/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1057, Accuracy: 9687/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1052, Accuracy: 9671/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0973, Accuracy: 9705/10000 (97%)\n",
      "\n",
      "model type M trail 2 starts\n",
      "\n",
      "Test set: Avg. loss: 0.4753, Accuracy: 8602/10000 (86%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2535, Accuracy: 9209/10000 (92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1864, Accuracy: 9439/10000 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1509, Accuracy: 9550/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1294, Accuracy: 9623/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1248, Accuracy: 9629/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1024, Accuracy: 9686/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1048, Accuracy: 9668/10000 (97%)\n",
      "\n",
      "model type M trail 3 starts\n",
      "\n",
      "Test set: Avg. loss: 0.4477, Accuracy: 8656/10000 (87%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2386, Accuracy: 9291/10000 (93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1797, Accuracy: 9482/10000 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1477, Accuracy: 9558/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1206, Accuracy: 9633/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1195, Accuracy: 9626/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1024, Accuracy: 9696/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0991, Accuracy: 9703/10000 (97%)\n",
      "\n",
      "model type M trail 4 starts\n",
      "\n",
      "Test set: Avg. loss: 0.4683, Accuracy: 8616/10000 (86%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2623, Accuracy: 9190/10000 (92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1722, Accuracy: 9489/10000 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1391, Accuracy: 9572/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1179, Accuracy: 9653/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1111, Accuracy: 9676/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0940, Accuracy: 9705/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0910, Accuracy: 9725/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = None\n",
    "optimizer = None\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "net_types = ['as_is']\n",
    "results = {}\n",
    "trials = 5\n",
    "for nt in net_types:\n",
    "  results[nt] = []\n",
    "  for i in range(trials):\n",
    "    print('model type', nt, 'trail', i, 'starts')\n",
    "    train_losses = []\n",
    "    train_counter = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    if nt == 'M':\n",
    "      net = FastUpdateNet()\n",
    "      optimizer = optim.SGD(net.get_parameters(), lr=learning_rate, momentum=momentum)\n",
    "    else:\n",
    "      net = TeacherNet()\n",
    "      optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      if nt == 'M':\n",
    "        train_fastNet(epoch, net)\n",
    "      else:\n",
    "        train_teacherNet(epoch, net)\n",
    "      test(net)\n",
    "    results[nt].append({'train_losses':train_losses, 'train_counter':train_counter, 'test_losses':test_losses, 'test_accuracies':test_accuracies})\n",
    "\n",
    "torch.save(results, './MNIST-result-as_is.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base tensor(97.4100) tensor(0.2647)\n",
      "M tensor(97.0200) tensor(0.2088)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "net_types = ['base', 'M']\n",
    "results = torch.load('./MNIST-result.pt')\n",
    "for nt in net_types:\n",
    "    total = 0\n",
    "    all_r = []\n",
    "    for r in results[nt]:\n",
    "        total = total + r['test_accuracies'][-1]\n",
    "        all_r.append(r['test_accuracies'][-1])\n",
    "    print(nt, total / len(results[nt]), torch.std(torch.tensor(all_r), dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(97.1500)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['base'][-2]['test_accuracies'][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.9786, Accuracy: 1234/10000 (12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2798, Accuracy: 9188/10000 (92%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hao/Documents/magic-m/fastNet_test.ipynb Cell 12\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/fastNet_test.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m test(fNet)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/fastNet_test.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/fastNet_test.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   train_teacherNet(epoch, fNet)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/fastNet_test.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m   test(fNet)\n",
      "\u001b[1;32m/home/hao/Documents/magic-m/fastNet_test.ipynb Cell 12\u001b[0m in \u001b[0;36mtrain_teacherNet\u001b[0;34m(epoch, network)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/fastNet_test.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_teacherNet\u001b[39m(epoch, network):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/fastNet_test.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m   network\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/fastNet_test.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m   \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/fastNet_test.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/fastNet_test.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     output \u001b[39m=\u001b[39m network(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:138\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, Any]:\n\u001b[1;32m    131\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m        index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m        tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     img, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index], \u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargets[index])\n\u001b[1;32m    140\u001b[0m     \u001b[39m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# to return a PIL Image\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tNet = torch.load('tNet.pt')\n",
    "fNet = FastUpdateNet(teacherNet=tNet)\n",
    "fNet.fc1 = torch.nn.Linear(784, 392)\n",
    "optimizer = optim.SGD(fNet.fc1.parameters(), lr=learning_rate, momentum=momentum)\n",
    "test(fNet)\n",
    "for epoch in range(1, 4):\n",
    "  train_teacherNet(epoch, fNet)\n",
    "  test(fNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNet = FastUpdateNet()\n",
    "fNet.mNet = torch.nn.Sequential(nn.Linear(392, 196), nn.ReLU(), nn.Linear(196, 98), nn.ReLU(), nn.Linear(98, 49), nn.ReLU())\n",
    "optimizer = optim.SGD(fNet.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/Documents/magic-m/model/network.py:131: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(o_4)\n",
      "/home/hao/.local/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3096, Accuracy: 891/10000 (9%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3962, Accuracy: 8793/10000 (88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2001, Accuracy: 9407/10000 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1858, Accuracy: 9440/10000 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1283, Accuracy: 9602/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "test(fNet)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train_teacherNet(epoch, fNet)\n",
    "  test(fNet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ee5443183715725406fd5246e657c0e511f90699501bc5e8c5d8d2b3c204bfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
