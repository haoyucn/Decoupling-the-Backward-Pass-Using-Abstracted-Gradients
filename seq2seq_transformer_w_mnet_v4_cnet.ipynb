{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch Multi30k tutorial\n",
    "complete with early stopping and testing with bleu score\n",
    "only concern is that original transformer arch was not tested on multi30k but on wmt2014 instead"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to download a mt dataset, https://pytorch.org/tutorials/beginner/translation_transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "import torchdata\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TGT_LANGUAGE = 'de'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "# pip install -U torchdata\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download de_core_news_sm\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 128\n",
    "\n",
    "# train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "# # train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct = 0\n",
    "# for _ in enumerate(train_dataloader):\n",
    "#     ct +=1 \n",
    "# ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BATCHES = 454 # for multi30k w bs=64\n",
    "DEVICE = 'cuda:1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "\n",
    "# m_shape = (64, 512, 512) # BS, EMB, EMB\n",
    "\n",
    "\n",
    "class GradSaver(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, sequentialOutput, saver):\n",
    "        # print('in forward in grad saver')\n",
    "        ctx.save_for_backward(x, saver, sequentialOutput)\n",
    "        return sequentialOutput.clone().detach()\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def backward(ctx, gradients):\n",
    "    #     # global lst_sqs_sum\n",
    "    #     # print('gradients in grad saver: ', gradients)\n",
    "        \n",
    "    #     x, saver, sequentialOutput, = ctx.saved_tensors\n",
    "    #     s = time.time()\n",
    "    #     m = torch.linalg.lstsq(x, sequentialOutput).solution\n",
    "    #     # print(x.shape)\n",
    "    #     # print(sequentialOutput.shape)\n",
    "    #     # print('saver dtype ', saver.dtype, saver.shape, saver.device)\n",
    "    #     # print('gradients dtype ', gradients.dtype, gradients.shape, gradients.device)\n",
    "    #     saver.grad = gradients.clone()#.cpu()\n",
    "    #     # print('in grad saver ', saver.grad)\n",
    "\n",
    "\n",
    "    #     # print(gradients.shape, m.shape)\n",
    "        \n",
    "    #     z = torch.matmul(gradients, m)#, None, None\n",
    "    #     # z = torch.bmm(gradients, m)\n",
    "    #     # lst_sqs_sum += (time.time() -s )\n",
    "    #     # z = torch.matmul(gradients, torch.transpose(m, 0, 1))#, None, None\n",
    "    #     # print('matmul result in grad saver: ', z)\n",
    "    #     return z, gradients, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, gradients):\n",
    "        global lst_sqs_sum\n",
    "        # print('gradients in grad saver: ', gradients)\n",
    "        \n",
    "        x, saver, sequentialOutput, = ctx.saved_tensors\n",
    "        x_shape = x.shape\n",
    "        x = torch.transpose(x, 0, 1) # reshaping to put batch first\n",
    "\n",
    "        s_shape = sequentialOutput.shape\n",
    "        sequentialOutput = torch.transpose(sequentialOutput, 0, 1) # reshaping to put batch first\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        s = time.time()\n",
    "        t = 1e-9\n",
    "        x_T = torch.transpose(x, -1, 1)\n",
    "        I = torch.eye(x.shape[1]).to('cuda:1')\n",
    "        pinv = torch.bmm(x_T, torch.inverse(torch.bmm(x, x_T) + t * I))\n",
    "        \n",
    "         \n",
    "        m = torch.bmm(pinv, sequentialOutput) # torch.transpose(pinv, -1, 1)\n",
    "\n",
    "        # print('here')\n",
    "        # print(m.shape)\n",
    "        # print(m[0,:,:] - m[1,:,:])\n",
    "\n",
    "        # print(x.shape)\n",
    "        # print(sequentialOutput.shape)\n",
    "        # print('saver dtype ', saver.dtype, saver.shape, saver.device)\n",
    "        # print('gradients dtype ', gradients.dtype, gradients.shape, gradients.device)\n",
    "        saver.grad = gradients.clone()#.cpu()\n",
    "        # print('in grad saver ', saver.grad)\n",
    "\n",
    "\n",
    "        # print(gradients.shape, m.shape)\n",
    "        # a = \n",
    "        # print(gradients.shape)\n",
    "        # print(m.shape)\n",
    "        gradients = torch.transpose(gradients, 0, 1) # reshaping method\n",
    "\n",
    "\n",
    "        z = torch.bmm(gradients, m)#, None, None\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------- Add bias vector here? z += MNet1_bias (where MNet1_bias = torch.rand((bs, dim)) init on creation of MNet) NOTE\n",
    "        # z = torch.bmm(gradients, m)\n",
    "        # lst_sqs_sum += (time.time() -s )\n",
    "        # z = torch.matmul(gradients, torch.transpose(m, 0, 1))#, None, None\n",
    "        # print('matmul result in grad saver: ', z)\n",
    "        # print(z.shape)\n",
    "        z = torch.transpose(z, 0, 1) # reshaping method\n",
    "        # print(z.shape, 'DONE')\n",
    "        # exit()\n",
    "\n",
    "        return z, None, None\n",
    "    \n",
    "class MNet_Transformer(torch.nn.Module):\n",
    "    def __init__(self, sequentialLayers, output_size):\n",
    "        super(MNet_Transformer, self).__init__()\n",
    "        self.layers = []\n",
    "        for l in sequentialLayers:\n",
    "            # if not isinstance(l, nn.ReLU):\n",
    "            if True: # insert all layers (attn, linear, dropout, norm)\n",
    "                # l.requires_grad = True\n",
    "                self.layers.append(l)\n",
    "        # self.layers.requires_grad = False\n",
    "        self.input_x = None\n",
    "        self.layersOutput = []\n",
    "        self.saver = torch.ones(output_size, dtype = self.layers[1].weight.dtype, requires_grad=True).to('cuda:1') # check dims of this (batch, output of M) \n",
    "        self.gradDiverge = GradSaver.apply\n",
    "\n",
    "    def getLayersOutput(self, x, mask, padding_mask):\n",
    "        self.layersOutput = []\n",
    "\n",
    "        x_clone = x.clone().detach()\n",
    "        x_clone.requires_grad = False\n",
    "        self.input_x = x_clone\n",
    "\n",
    "        lo = x_clone\n",
    "        for l in self.layers:\n",
    "            if isinstance(l, nn.MultiheadAttention):\n",
    "                if padding_mask is not None:\n",
    "                    lo = l(lo,lo,lo, attn_mask=mask, key_padding_mask=padding_mask)[0] # using attn output only not the attn output weights\n",
    "                else:\n",
    "                    lo = l(lo,lo,lo, attn_mask=mask)[0] # using attn output only not the attn output weights\n",
    "            else:\n",
    "                lo = l(lo)\n",
    "            # if lo.requires_grad == False:\n",
    "                # print('error')\n",
    "                # print(l)\n",
    "\n",
    "        return lo\n",
    "\n",
    "    def forward(self, x, mask, padding_mask):\n",
    "        x_clone = x.clone().detach()\n",
    "        x_clone.requires_grad = False\n",
    "        self.sequentialOutput = self.getLayersOutput(x_clone, mask, padding_mask)\n",
    "        # self.saver = sequentialOutput.clone().detach()\n",
    "        if self.saver.shape != self.sequentialOutput.shape:\n",
    "            self.saver = torch.ones(self.sequentialOutput.shape).to('cuda:1')\n",
    "        # print('self.sequentialOutput.requires_grad ', self.sequentialOutput.requires_grad)\n",
    "        # m = torch.linalg.lstsq(x_clone, self.sequentialOutput).solution.detach()\n",
    "        # o = F.linear(x, torch.transpose(m, 0, 1))\n",
    "        t = self.gradDiverge(x, self.sequentialOutput.clone().detach(), self.saver)\n",
    "        \n",
    "        # print('t, ', t.grad)\n",
    "        return t\n",
    "\n",
    "    def backwardHidden(self):\n",
    "        # print('in MNet, ', self.saver.grad)\n",
    "        # print('seqoutput grad', self.sequentialOutput.grad)\n",
    "        self.sequentialOutput.backward(gradient = self.saver.grad.clone().detach())\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        ps = []\n",
    "        ps.append(self.layers[0].out_proj.weight) # self attn\n",
    "        ps.append(self.layers[0].out_proj.bias)\n",
    "\n",
    "        for l in self.layers:\n",
    "            if hasattr(l, 'weight') and hasattr(l, 'bias'):\n",
    "                ps.append(l.weight)\n",
    "                ps.append(l.bias)\n",
    "        return ps\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seq2seq transformer arch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class CustomSequential(nn.Module):\n",
    "    def __init__(self, encoder_layers) -> None:\n",
    "        super(CustomSequential, self).__init__()\n",
    "        self.encoder_layers = encoder_layers\n",
    "        modules = []\n",
    "        for i in range(len(encoder_layers)):\n",
    "            modules.append(encoder_layers[i].self_attn)\n",
    "            modules.append(encoder_layers[i].linear1)\n",
    "            modules.append(encoder_layers[i].dropout)\n",
    "            modules.append(encoder_layers[i].linear2)\n",
    "            modules.append(encoder_layers[i].norm1)\n",
    "            modules.append(encoder_layers[i].norm2)\n",
    "            modules.append(encoder_layers[i].dropout)\n",
    "            modules.append(encoder_layers[i].dropout)\n",
    "            # try to add the layernorms too\n",
    "\n",
    "        self.custom_sequential = nn.Sequential(*modules)\n",
    "\n",
    "    # def forward(self, src, src_mask, src_padding_mask):\n",
    "    #     t = self.encoder_layers[i].self_attn(src, src_mask)\n",
    "\n",
    "    #     return t\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer_N6(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(CustomTransformer_N6, self).__init__()\n",
    "        self.model = Transformer(d_model=d_model,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        # print('pre mnet init')\n",
    "        # self.MNet1 = MNet(nn.Sequential(self.model.encoder.layers[1], self.model.encoder.layers[2][0], self.model.encoder.layers[3][0], self.model.encoder.layers[4][0]), (self.batch_size, 512))\n",
    "        # self.MNet2 = MNet(nn.Sequential(self.model.encoder.layers[6], self.model.encoder.layers[7], self.model.encoder.layers[8], self.model.encoder.layers[9]), (self.batch_size, 512))\n",
    "        self.MNet1 = MNet_Transformer(CustomSequential(self.model.encoder.layers[1:5]).custom_sequential, (self.batch_size, 512))\n",
    "        # self.MNet2 = MNet_Transformer(CustomSequential(self.model.encoder.layers[6:-2]).custom_sequential, (self.batch_size, 512))\n",
    "\n",
    "        # print('post mnet init')\n",
    "\n",
    "    def fi(self, encoder_layer, src, src_mask, src_padding_mask,):\n",
    "        # this works and is the proper way to apply self attn in the encoder and to apply the masks (i think)\n",
    "        if src_padding_mask is not None:\n",
    "            t = encoder_layer.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_padding_mask)[0] # not returning the attn output weights\n",
    "        else:\n",
    "            t = encoder_layer.self_attn(src, src, src, attn_mask=src_mask,)[0] # not returning the attn output weights\n",
    "        # print('in fi')\n",
    "        # print(len(t))\n",
    "        # print(t)\n",
    "        t = encoder_layer.linear1(t)\n",
    "        t = encoder_layer.dropout(t)\n",
    "        t = encoder_layer.linear2(t)\n",
    "        t = encoder_layer.norm1(t)\n",
    "        t = encoder_layer.norm2(t)\n",
    "        t = encoder_layer.dropout(t)\n",
    "        t = encoder_layer.dropout(t)\n",
    "        return t\n",
    "\n",
    "    def custom_encode(self, src, src_mask, src_padding_mask):\n",
    "        t = src\n",
    "        t = self.fi(self.model.encoder.layers[0], src, src_mask, src_padding_mask)\n",
    "        # t = self.model.encoder.layers[0](t, attn_mask=src_mask, key_padding_mask=src_padding_mask)\n",
    "        # print('here', t.shape)\n",
    "        # print(t)\n",
    "        t = self.MNet1(t, src_mask, src_padding_mask)\n",
    "        # t = self.fi(self.model.encoder.layers[-2], t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[-1], src, src_mask, src_padding_mask)\n",
    "\n",
    "        return t\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        # mem = self.model.encoder(src) # apply M\n",
    "        t = src\n",
    "        t = self.fi(self.model.encoder.layers[0], src, src_mask, src_padding_mask)\n",
    "        # t = self.model.encoder.layers[0](t, attn_mask=src_mask, key_padding_mask=src_padding_mask)\n",
    "        # print('here', t.shape)\n",
    "        # print(t)\n",
    "        t = self.MNet1(t, src_mask, src_padding_mask)\n",
    "        # t = self.fi(self.model.encoder.layers[5], t, src_mask, src_padding_mask)\n",
    "        # t = self.MNet2(t, src_mask, src_padding_mask)\n",
    "        # print('forward in custom transformer, ', t.requires_grad)\n",
    "        # t = self.fi(self.model.encoder.layers[-2], t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[-1], t, src_mask, src_padding_mask)\n",
    "\n",
    "\n",
    "\n",
    "        mem = t #(in this case just the src padding mask which is boolean)\n",
    "        output = self.model.decoder(tgt, mem, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        ps = []\n",
    "        mnet1Ps = self.MNet1.get_parameters()\n",
    "        # mnet2Ps = self.MNet2.get_parameters()\n",
    "        \n",
    "\n",
    "        # add params outside of layers abstracted by Ms in encoder and decoder\n",
    "        kept_encoder_layers = [self.model.encoder.layers[0], self.model.encoder.layers[-1]]\n",
    "        for kept_layer in kept_encoder_layers:\n",
    "            ps.append(kept_layer.self_attn.out_proj.weight)\n",
    "            ps.append(kept_layer.self_attn.out_proj.bias)\n",
    "            ps.append(kept_layer.linear1.weight)\n",
    "            ps.append(kept_layer.linear1.bias)\n",
    "            ps.append(kept_layer.linear2.weight)\n",
    "            ps.append(kept_layer.linear2.bias)\n",
    "\n",
    "        for block in self.model.decoder.layers:\n",
    "            ps.append(block.self_attn.out_proj.weight)\n",
    "            ps.append(block.self_attn.out_proj.bias)\n",
    "            ps.append(block.multihead_attn.out_proj.weight)\n",
    "            ps.append(block.multihead_attn.out_proj.bias)\n",
    "            ps.append(block.linear1.weight)\n",
    "            ps.append(block.linear1.bias)\n",
    "            ps.append(block.linear2.weight)\n",
    "            ps.append(block.linear2.bias)\n",
    "            \n",
    "\n",
    "        for p in mnet1Ps:\n",
    "            ps.append(p)\n",
    "        # for p in mnet2Ps:\n",
    "        #     ps.append(p)\n",
    "        return ps\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer_N6(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer_N6, self).__init__()\n",
    "        self.transformer = CustomTransformer_N6(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size) # not returning the attn output weights\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        # print('src_emb.shape ', src_emb.shape)\n",
    "        # print('src_mask.shape ', src_mask.shape)\n",
    "        # print('src_emb ', src_emb)\n",
    "        # print('src_mask ', src_mask)\n",
    "        # exit(0)\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask) # find out how masking is applied within forward method\n",
    "        return self.generator(outs)\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        ps = []\n",
    "        transformer_ps = self.transformer.get_parameters()\n",
    "        ps.extend(transformer_ps)\n",
    "        # ps.append(self.generator.weight)\n",
    "        # ps.append(self.generator.bias)\n",
    "        # ps.append(self.src_tok_emb.embedding.weight)\n",
    "        # ps.append(self.tgt_tok_emb.embedding.weight)\n",
    "\n",
    "        return ps\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer_N12(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(CustomTransformer_N12, self).__init__()\n",
    "        self.model = Transformer(d_model=d_model,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        # print('pre mnet init')\n",
    "        # self.MNet1 = MNet(nn.Sequential(self.model.encoder.layers[1], self.model.encoder.layers[2][0], self.model.encoder.layers[3][0], self.model.encoder.layers[4][0]), (self.batch_size, 512))\n",
    "        # self.MNet2 = MNet(nn.Sequential(self.model.encoder.layers[6], self.model.encoder.layers[7], self.model.encoder.layers[8], self.model.encoder.layers[9]), (self.batch_size, 512))\n",
    "        self.MNet1 = MNet_Transformer(CustomSequential(self.model.encoder.layers[1:5]).custom_sequential, (self.batch_size, 512))\n",
    "        self.MNet2 = MNet_Transformer(CustomSequential(self.model.encoder.layers[6:-2]).custom_sequential, (self.batch_size, 512))\n",
    "\n",
    "        # print('post mnet init')\n",
    "\n",
    "    def fi(self, encoder_layer, src, src_mask, src_padding_mask,):\n",
    "        # this works and is the proper way to apply self attn in the encoder and to apply the masks (i think)\n",
    "        if src_padding_mask is not None:\n",
    "            t = encoder_layer.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_padding_mask)[0] # not returning the attn output weights\n",
    "        else:\n",
    "            t = encoder_layer.self_attn(src, src, src, attn_mask=src_mask,)[0] # not returning the attn output weights\n",
    "        # print('in fi')\n",
    "        # print(len(t))\n",
    "        # print(t)\n",
    "        t = encoder_layer.linear1(t)\n",
    "        t = encoder_layer.dropout(t)\n",
    "        t = encoder_layer.linear2(t)\n",
    "        t = encoder_layer.norm1(t)\n",
    "        t = encoder_layer.norm2(t)\n",
    "        t = encoder_layer.dropout(t)\n",
    "        t = encoder_layer.dropout(t)\n",
    "        return t\n",
    "\n",
    "    def custom_encode(self, src, src_mask, src_padding_mask):\n",
    "        t = src\n",
    "        t = self.fi(self.model.encoder.layers[0], src, src_mask, src_padding_mask)\n",
    "        # t = self.model.encoder.layers[0](t, attn_mask=src_mask, key_padding_mask=src_padding_mask)\n",
    "        # print('here', t.shape)\n",
    "        # print(t)\n",
    "        t = self.MNet1(t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[5], src, src_mask, src_padding_mask)\n",
    "        t = self.MNet2(t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[-2], t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[-1], src, src_mask, src_padding_mask)\n",
    "\n",
    "        return t\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        # mem = self.model.encoder(src) # apply M\n",
    "        t = src\n",
    "        t = self.fi(self.model.encoder.layers[0], src, src_mask, src_padding_mask)\n",
    "        # t = self.model.encoder.layers[0](t, attn_mask=src_mask, key_padding_mask=src_padding_mask)\n",
    "        # print('here', t.shape)\n",
    "        # print(t)\n",
    "        t = self.MNet1(t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[5], t, src_mask, src_padding_mask)\n",
    "        t = self.MNet2(t, src_mask, src_padding_mask)\n",
    "        # print('forward in custom transformer, ', t.requires_grad)\n",
    "        t = self.fi(self.model.encoder.layers[-2], t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[-1], t, src_mask, src_padding_mask)\n",
    "\n",
    "\n",
    "\n",
    "        mem = t #(in this case just the src padding mask which is boolean)\n",
    "        output = self.model.decoder(tgt, mem, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        ps = []\n",
    "        mnet1Ps = self.MNet1.get_parameters()\n",
    "        mnet2Ps = self.MNet2.get_parameters()\n",
    "        \n",
    "\n",
    "        # add params outside of layers abstracted by Ms in encoder and decoder\n",
    "        kept_encoder_layers = [self.model.encoder.layers[0], self.model.encoder.layers[5], self.model.encoder.layers[-1]]\n",
    "        for kept_layer in kept_encoder_layers:\n",
    "            ps.append(kept_layer.self_attn.out_proj.weight)\n",
    "            ps.append(kept_layer.self_attn.out_proj.bias)\n",
    "            ps.append(kept_layer.linear1.weight)\n",
    "            ps.append(kept_layer.linear1.bias)\n",
    "            ps.append(kept_layer.linear2.weight)\n",
    "            ps.append(kept_layer.linear2.bias)\n",
    "\n",
    "        for block in self.model.decoder.layers:\n",
    "            ps.append(block.self_attn.out_proj.weight)\n",
    "            ps.append(block.self_attn.out_proj.bias)\n",
    "            ps.append(block.multihead_attn.out_proj.weight)\n",
    "            ps.append(block.multihead_attn.out_proj.bias)\n",
    "            ps.append(block.linear1.weight)\n",
    "            ps.append(block.linear1.bias)\n",
    "            ps.append(block.linear2.weight)\n",
    "            ps.append(block.linear2.bias)\n",
    "            \n",
    "\n",
    "        for p in mnet1Ps:\n",
    "            ps.append(p)\n",
    "        for p in mnet2Ps:\n",
    "            ps.append(p)\n",
    "        return ps\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer_N12(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer_N12, self).__init__()\n",
    "        self.transformer = CustomTransformer_N12(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size) # not returning the attn output weights\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        # print('src_emb.shape ', src_emb.shape)\n",
    "        # print('src_mask.shape ', src_mask.shape)\n",
    "        # print('src_emb ', src_emb)\n",
    "        # print('src_mask ', src_mask)\n",
    "        # exit(0)\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask) # find out how masking is applied within forward method\n",
    "        return self.generator(outs)\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        ps = []\n",
    "        transformer_ps = self.transformer.get_parameters()\n",
    "        ps += transformer_ps\n",
    "        ps.append(self.generator.weight)\n",
    "        ps.append(self.generator.bias)\n",
    "        ps.append(self.src_tok_emb.embedding.weight)\n",
    "        ps.append(self.tgt_tok_emb.embedding.weight)\n",
    "\n",
    "        return ps\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 2048\n",
    "BATCH_SIZE = 64\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "\n",
    "transformer = Seq2SeqTransformer_N6(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "# transformer = Seq2SeqTransformer_N12(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "#                                  NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to('cuda:1')\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "# lr = 0.0007 \n",
    "lr = 0.0001\n",
    "# optimizer = torch.optim.Adam(transformer.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "# optimizer = torch.optim.Adam(transformer.get_parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "print(len(transformer.get_parameters()))\n",
    "print(sum(1 for _ in transformer.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "all_params = []\n",
    "for n,p in transformer.named_parameters():\n",
    "    if 'encoder.layers' in n:\n",
    "        if 'encoder.layers.0' in n or 'encoder.layers.5' in n: # for N_enconder=6\n",
    "            all_params.append(p)\n",
    "    else:\n",
    "        all_params.append(p)\n",
    "    # print(n)\n",
    "    ct += 1\n",
    "len(all_params)\n",
    "all_params.extend(transformer.get_parameters())\n",
    "len(all_params)\n",
    "\n",
    "all_params = [*set(all_params)]\n",
    "print(len(all_params))\n",
    "optimizer = torch.optim.Adam(all_params, lr=lr, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_batches():\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    ct = 0\n",
    "    for _ in enumerate(train_dataloader):\n",
    "        ct +=1 \n",
    "    return ct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_BATCH_SIZE = 13 # for bs=64 and on multi30k (using attn is all you need setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from threading import Thread\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def update_lr(step_num):\n",
    "    num_warmup = 4000\n",
    "    return (EMB_SIZE)**-0.5 * min(step_num**(-0.5), step_num * num_warmup**(-1.5))\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    saved_losses = []\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    step = 0\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        # if src.shape[1] != BATCH_SIZE:\n",
    "        #     print('not skipping batch of size ',src.shape[1])\n",
    "        #     # continue \n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        step_num = epoch * step\n",
    "        if epoch > 1 and step == 0:\n",
    "            step_num = epoch * NUM_BATCHES\n",
    "        optim.param_groups[0]['lr'] = update_lr(step_num)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        model.transformer.MNet1.backwardHidden()\n",
    "        model.transformer.MNet2.backwardHidden()\n",
    "\n",
    "        # a = model.transformer.MNet1.layers[0].out_proj.weight.grad\n",
    "        # # print('grad of self attn in MNet1, ', a)\n",
    "\n",
    "        # a = model.transformer.MNet1.layers[1].weight.grad\n",
    "        # print('grad of linear1 in MNet1, ', a)\n",
    "\n",
    "        # # print()\n",
    "        # a = model.transformer.model.encoder.layers[0].linear1.weight.grad\n",
    "        # print('grad of linear in regular layer0, ', a)\n",
    "        \n",
    "        \n",
    "        # a = model.transformer.model.encoder.layers[-1].linear1.weight.grad\n",
    "        # print('grad of linear in regular layer-1, ', a)\n",
    "\n",
    "        # print(\"FINISHED ONE BATCH!\")\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        saved_losses.append(loss.item())\n",
    "        step += 1\n",
    "\n",
    "    return losses / len(list(train_dataloader)), saved_losses\n",
    "\n",
    "def update1(network):\n",
    "  network.MNet1.backwardHidden() \n",
    "\n",
    "def update2(network):\n",
    "  network.MNet2.backwardHidden() \n",
    "\n",
    "def train_epoch_parallel(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    saved_losses = []\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    step = 1\n",
    "    minibatch_counter = 0\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        # if src.shape[1] != BATCH_SIZE:\n",
    "        #     print('not skipping batch of size ',src.shape[1])\n",
    "        #     # continue \n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        # exit()\n",
    "\n",
    "        step_num = int((epoch-1) * (NUM_BATCHES // LARGE_BATCH_SIZE)) + step\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        # print('before general backward')\n",
    "        loss.backward()\n",
    "\n",
    "        # check if first encoder layer has grad\n",
    "        # g = model.transformer.model.encoder.layers[5].linear1.weight.grad\n",
    "        # print('g ', g)\n",
    "        # parallelized update\n",
    "        p1 = Thread(target=update1, args=[model.transformer]) # start two independent threads\n",
    "        # p2 = Thread(target=update2, args=[model.transformer]) # start two independent threads\n",
    "        \n",
    "        p1.start()\n",
    "        # p2.start()\n",
    "        if hasattr(model.transformer, 'MNet2'): # in the case n = 6 there is no mnet2\n",
    "            model.transformer.MNet2.backwardHidden()\n",
    "\n",
    "\n",
    "        p1.join()\n",
    "        # p2.join()\n",
    "\n",
    "        # print('before mnet backwards')\n",
    "        # model.transformer.MNet1.backwardHidden()\n",
    "        \n",
    "        # wait for both MNets to finish updating\n",
    "\n",
    "        # a = model.transformer.MNet1.layers[0].out_proj.weight.grad\n",
    "        # print('grad of self attn in MNet1, ', a)\n",
    "\n",
    "        \n",
    "        losses += loss.item()\n",
    "        saved_losses.append(loss.item())\n",
    "        minibatch_counter += 1\n",
    "        \n",
    "        if minibatch_counter > LARGE_BATCH_SIZE - 1: \n",
    "            minibatch_counter = 0\n",
    "            # update the lr and the weights\n",
    "            # print(step)\n",
    "            # print((NUM_BATCHES // LARGE_BATCH_SIZE))\n",
    "            # print(epoch-1)\n",
    "            # print(int(epoch-1 * (NUM_BATCHES / LARGE_BATCH_SIZE)) + step)\n",
    "            # print(int((epoch-1) * (NUM_BATCHES / LARGE_BATCH_SIZE)) + 0)\n",
    "            # print( int(epoch * (NUM_BATCHES / LARGE_BATCH_SIZE)) + step )\n",
    "\n",
    "            # print(type(step_num))\n",
    "            # print((step_num))\n",
    "            optimizer.param_groups[0]['lr'] = update_lr(step_num)\n",
    "            optimizer.step()\n",
    "            # reset grad\n",
    "            optimizer.zero_grad()\n",
    "            step += 1\n",
    "\n",
    "    # update lr and weights as long as they weren't JUST updated\n",
    "    if minibatch_counter > 0:\n",
    "        # update the lr and the weights\n",
    "        optimizer.param_groups[0]['lr'] = update_lr(step_num)\n",
    "        optimizer.step()\n",
    "        # reset grad\n",
    "        optimizer.zero_grad()\n",
    "        step += 1\n",
    "\n",
    "    return losses / len(list(train_dataloader)), saved_losses\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    saved_losses_val = []\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "        saved_losses_val.append(loss.item())\n",
    "\n",
    "    return losses / len(list(val_dataloader)), saved_losses_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps:  27240\n",
      "Epoch: 1, Train loss: 9.610, Val loss: 9.266, Epoch time = 41.145s\n",
      "Epoch: 2, Train loss: 9.143, Val loss: 8.916, Epoch time = 40.675s\n",
      "Epoch: 3, Train loss: 8.784, Val loss: 8.500, Epoch time = 41.190s\n",
      "Epoch: 4, Train loss: 8.282, Val loss: 7.922, Epoch time = 41.374s\n",
      "Epoch: 5, Train loss: 7.643, Val loss: 7.235, Epoch time = 40.889s\n",
      "Epoch: 6, Train loss: 6.969, Val loss: 6.605, Epoch time = 41.441s\n",
      "Epoch: 7, Train loss: 6.356, Val loss: 6.056, Epoch time = 41.453s\n",
      "Epoch: 8, Train loss: 5.852, Val loss: 5.649, Epoch time = 41.533s\n",
      "Epoch: 9, Train loss: 5.484, Val loss: 5.365, Epoch time = 41.573s\n",
      "Epoch: 10, Train loss: 5.208, Val loss: 5.133, Epoch time = 41.682s\n",
      "Epoch: 11, Train loss: 4.969, Val loss: 4.912, Epoch time = 41.229s\n",
      "Epoch: 12, Train loss: 4.752, Val loss: 4.712, Epoch time = 41.759s\n",
      "Epoch: 13, Train loss: 4.559, Val loss: 4.555, Epoch time = 41.696s\n",
      "Epoch: 14, Train loss: 4.389, Val loss: 4.389, Epoch time = 41.438s\n",
      "Epoch: 15, Train loss: 4.240, Val loss: 4.286, Epoch time = 41.767s\n",
      "Epoch: 16, Train loss: 4.110, Val loss: 4.176, Epoch time = 41.553s\n",
      "Epoch: 17, Train loss: 4.015, Val loss: 4.117, Epoch time = 41.754s\n",
      "Epoch: 18, Train loss: 3.932, Val loss: 4.020, Epoch time = 41.723s\n",
      "Epoch: 19, Train loss: 3.815, Val loss: 3.951, Epoch time = 41.774s\n",
      "Epoch: 20, Train loss: 3.723, Val loss: 3.898, Epoch time = 41.823s\n",
      "Epoch: 21, Train loss: 3.641, Val loss: 3.857, Epoch time = 41.828s\n",
      "Epoch: 22, Train loss: 3.580, Val loss: 3.860, Epoch time = 41.781s\n",
      "Epoch: 23, Train loss: 3.521, Val loss: 3.774, Epoch time = 41.702s\n",
      "Epoch: 24, Train loss: 3.477, Val loss: 3.755, Epoch time = 41.583s\n",
      "Epoch: 25, Train loss: 3.423, Val loss: 3.756, Epoch time = 41.335s\n",
      "Epoch: 26, Train loss: 3.371, Val loss: 3.744, Epoch time = 41.687s\n",
      "Epoch: 27, Train loss: 3.316, Val loss: 3.713, Epoch time = 41.737s\n",
      "Epoch: 28, Train loss: 3.275, Val loss: 3.668, Epoch time = 41.640s\n",
      "Epoch: 29, Train loss: 3.242, Val loss: 3.668, Epoch time = 41.681s\n",
      "Epoch: 30, Train loss: 3.185, Val loss: 3.648, Epoch time = 41.736s\n",
      "Epoch: 31, Train loss: 3.135, Val loss: 3.665, Epoch time = 41.573s\n",
      "Epoch: 32, Train loss: 3.118, Val loss: 3.649, Epoch time = 41.605s\n",
      "Epoch: 33, Train loss: 3.074, Val loss: 3.646, Epoch time = 41.090s\n",
      "Epoch: 34, Train loss: 3.026, Val loss: 3.638, Epoch time = 41.661s\n",
      "Epoch: 35, Train loss: 2.967, Val loss: 3.639, Epoch time = 41.703s\n",
      "Epoch: 36, Train loss: 2.940, Val loss: 3.634, Epoch time = 41.554s\n",
      "Epoch: 37, Train loss: 2.890, Val loss: 3.616, Epoch time = 41.699s\n",
      "Epoch: 38, Train loss: 2.887, Val loss: 3.676, Epoch time = 41.722s\n",
      "Epoch: 39, Train loss: 2.823, Val loss: 3.609, Epoch time = 41.628s\n",
      "Epoch: 40, Train loss: 2.757, Val loss: 3.615, Epoch time = 41.719s\n",
      "Epoch: 41, Train loss: 2.710, Val loss: 3.612, Epoch time = 41.741s\n",
      "Epoch: 42, Train loss: 2.670, Val loss: 3.628, Epoch time = 41.746s\n",
      "Epoch: 43, Train loss: 2.634, Val loss: 3.634, Epoch time = 41.702s\n",
      "Epoch: 44, Train loss: 2.603, Val loss: 3.659, Epoch time = 41.962s\n",
      "Epoch: 45, Train loss: 2.609, Val loss: 3.700, Epoch time = 41.819s\n",
      "Epoch: 46, Train loss: 2.593, Val loss: 3.706, Epoch time = 41.596s\n",
      "Epoch: 47, Train loss: 2.605, Val loss: 3.767, Epoch time = 41.637s\n",
      "Epoch: 48, Train loss: 2.548, Val loss: 3.742, Epoch time = 41.524s\n",
      "Epoch: 49, Train loss: 2.489, Val loss: 3.785, Epoch time = 41.487s\n",
      "Epoch: 50, Train loss: 2.446, Val loss: 3.777, Epoch time = 41.607s\n",
      "Epoch: 51, Train loss: 2.401, Val loss: 3.825, Epoch time = 41.623s\n",
      "Epoch: 52, Train loss: 2.361, Val loss: 3.845, Epoch time = 41.604s\n",
      "Epoch: 53, Train loss: 2.322, Val loss: 3.909, Epoch time = 41.553s\n",
      "Epoch: 54, Train loss: 2.284, Val loss: 3.906, Epoch time = 41.500s\n",
      "Epoch: 55, Train loss: 2.234, Val loss: 3.961, Epoch time = 41.453s\n",
      "Epoch: 56, Train loss: 2.193, Val loss: 3.922, Epoch time = 41.726s\n",
      "Epoch: 57, Train loss: 2.144, Val loss: 3.998, Epoch time = 41.319s\n",
      "Epoch: 58, Train loss: 2.107, Val loss: 3.993, Epoch time = 41.114s\n",
      "Epoch: 59, Train loss: 2.063, Val loss: 4.025, Epoch time = 41.594s\n",
      "Epoch: 60, Train loss: 2.019, Val loss: 4.030, Epoch time = 41.558s\n",
      "Transformer model saved (seq2seq_transformer_multi30k_weights_m_dropout_epochs=60.pt)\n",
      "Trained for 60 epochs in 2553.678714990616 seconds\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "NUM_EPOCHS = 60  # quarter of way training according to attn paper (looks like it starts overfitting pretty early)\n",
    "DEVICE = 'cuda:1'\n",
    "# PATH = 'seq2seq_transformer_multi30k_weights.pt'\n",
    "PATH = 'seq2seq_transformer_multi30k_weights_m_tmp.pt'\n",
    "NUM_BATCHES = get_num_batches() \n",
    "NUM_STEPS = NUM_EPOCHS * NUM_BATCHES\n",
    "print('Num steps: ', NUM_STEPS)\n",
    "\n",
    "try:\n",
    "    # raise Exception\n",
    "    transformer.load_state_dict(torch.load(PATH))\n",
    "    print('Transformer model weights loaded')\n",
    "except Exception:\n",
    "    # early_stopper = EarlyStopper(patience=3, min_delta=0.025)\n",
    "    t0 = time.time()\n",
    "    training_losses_to_plot = []\n",
    "    val_losses_to_plot = []\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        start_time = timer()\n",
    "        train_loss, saved_losses_train = train_epoch_parallel(transformer, optimizer)\n",
    "        # train_loss, saved_losses_train = train_epoch(transformer, optimizer)\n",
    "        end_time = timer()\n",
    "        val_loss, saved_losses_val = evaluate(transformer)\n",
    "        # print('M computation TIME PER EPOCH: ', lst_sqs_sum)\n",
    "        print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "        lst_sqs_sum = 0\n",
    "        if epoch > 25:\n",
    "            torch.save(transformer.state_dict(), f'tmp_transformer_base_w_MNet_weights_epoch={epoch}.pt')\n",
    "\n",
    "\n",
    "        if len(val_losses_to_plot) > 0 and val_loss < min(val_losses_to_plot):\n",
    "            best_model = transformer\n",
    "            torch.save(best_model.state_dict(), 'tmp_transformer_base_w_MNet_weights.pt')\n",
    "            # keep the model with lowest val loss\n",
    "        \n",
    "        \n",
    "        training_losses_to_plot.append(train_loss)\n",
    "        val_losses_to_plot.append(val_loss)\n",
    "\n",
    "\n",
    "        # if early_stopper.early_stop(val_loss):\n",
    "        #     print(\"<EARLY STOP> model done training.\")             \n",
    "        #     break\n",
    "    tf = time.time()\n",
    "    PATH = f'seq2seq_transformer_multi30k_weights_m_tmp_epochs={epoch}.pt'\n",
    "    torch.save(transformer.state_dict(), PATH)\n",
    "    print(f'Transformer model saved ({PATH})')\n",
    "    print(f'Trained for {NUM_EPOCHS} epochs in {tf - t0} seconds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve training further, can use adaptive learning rate schedule as described in sec 5.3 and also label smoothing as described in sec 5.4 - done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = best_model\n",
    "torch.save(transformer.state_dict(), 'MNet_transformer_best_val_loss_tmp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num steps:  27240\n"
     ]
    }
   ],
   "source": [
    "print(\"total num steps: \", NUM_EPOCHS * NUM_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_state_dict(torch.load('MNet_transformer_best_val_loss_tmp.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plotting curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "s = (len(training_losses_to_plot) // len(val_losses_to_plot)) \n",
    "\n",
    "# plt.plot(np.arange(0, len(training_losses_to_plot), 10), training_losses_to_plot[::10])\n",
    "plt.plot(range(NUM_EPOCHS), training_losses_to_plot, label='Training')\n",
    "plt.plot(range(NUM_EPOCHS), val_losses_to_plot, label='Validation')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import bleu_score\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encoder(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode_MNet(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "    # print(src.shape)\n",
    "    src_emb = model.positional_encoding(model.src_tok_emb(src))\n",
    "    # print(src_emb.shape)\n",
    "    memory = model.transformer.custom_encode(src_emb, src_mask, None)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        ys_emb = model.positional_encoding(model.tgt_tok_emb(ys))\n",
    "        out = model.transformer.model.decoder(ys_emb, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate_MNet(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    # print('in translate, ', src.shape)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode_MNet(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "def test(model,):\n",
    "    test_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    \n",
    "    # compute the avg bleu score\n",
    "    bleu = 0\n",
    "    ct = 0\n",
    "    for data_sample in test_iter:\n",
    "        ct += 1\n",
    "        # print(data_sample)\n",
    "        src = data_sample[0]\n",
    "        tgt = data_sample[1]\n",
    "        pred = translate_MNet(model, src)\n",
    "        bleu += bleu_score([pred], [tgt])\n",
    "    return bleu / ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyle/pythonenvs/venv38/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/kyle/pythonenvs/venv38/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/kyle/pythonenvs/venv38/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "# transformer.load_state_dict(torch.load('seq2seq_transformer_multi30k_weights_epochs=21.pt'))\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "test_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "de = []\n",
    "de_refs= []\n",
    "en = []\n",
    "de_preds = []\n",
    "# de_preds2 = []\n",
    "sentence_scores = []\n",
    "for data_sample in test_iter:\n",
    "    if len(data_sample[0]) < 3 or len(data_sample[1]) < 3:\n",
    "        print('here')\n",
    "        continue\n",
    "    \n",
    "    en.append(data_sample[0].split())\n",
    "    de.append(data_sample[1].split())\n",
    "    de_refs.append([data_sample[1].split()])\n",
    "    \n",
    "    pred = translate_MNet(transformer, data_sample[0])\n",
    "    pred_split = pred.split()\n",
    "    # if pred_split[-1] == '.':\n",
    "    #     pred_split = pred_split[:-1] # trim periods\n",
    "    de_preds.append(pred_split)\n",
    "    sentence_scores.append(sentence_bleu(de[-1], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008756459383859189"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu(de_refs, de_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg BLEU EN->DE scores (with MNet): \n",
      "0.10175191681396906\n"
     ]
    }
   ],
   "source": [
    "print('avg BLEU EN->DE scores (with MNet): ')\n",
    "\n",
    "print(sum(sentence_scores) / len(sentence_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Eine', 'brnette', 'Frau', 'steht', 'auf', 'dem', 'Gehweg', 'und', 'blickt', 'die', 'Strae', 'hinunter.'], ['Eine', 'Gruppe', 'von', 'drei', 'Freunden', 'unterhlt', 'sich', 'in', 'einem', 'Haus.'], ['Zwei', 'Chinesen', 'stehen', 'an', 'einer', 'Wandtafel.'], ['Eine', 'Person', 'in', 'Bluejeans', 'und', 'rotem', 'Pullover', 'biegt', 'um', 'die', 'Ecke', 'einer', 'Ziegelmauer.'], ['Bauern', 'betreiben', 'tagsber', 'ihre', 'Landwirtschaft.'], ['Bei', 'einer', 'Art', 'Jahrmarkt', 'stellt', 'ein', 'Mann', 'Zuckerwatte', 'her.'], ['Eine', 'Gruppe', 'von', 'Polizisten', 'steht', 'vor', 'einem', 'Bus.'], ['Eine', 'ltere', 'weihaarige', 'Frau', 'sieht', 'in', 'ihre', 'Kasse', 'und', 'blickt', 'durch', 'ihre', 'Brillenglser', 'hindurch.'], ['Zwei', 'Mnner', 'stehen', 'an', 'Telefonzellen', 'im', 'Freien.'], ['Zwei', 'Frauen', 'in', 'Rot', 'und', 'ein', 'Mann,', 'der', 'aus', 'einer', 'transportablen', 'Toilette', 'kommt.']]\n",
      "[['Ein', 'Mann', 'fhrt', 'auf', 'einem', 'Fahrrad', 'durch', 'eine', 'Strae', '.'], ['Ein', 'Mann', 'fhrt', 'auf', 'einem', 'Skateboard', 'auf', 'einem', 'Feldweg', '.'], ['Ein', 'Mann', 'fhrt', 'auf', 'einem', 'Fahrrad', 'durch', 'eine', 'Strae', '.'], ['Ein', 'Mann', 'fhrt', 'auf', 'einem', 'Skateboard', 'auf', 'einem', 'Feld', '.'], ['Ein', 'Mann', 'fhrt', 'auf', 'einem', 'Fahrrad', 'durch', 'die', 'Luft', '.'], ['Ein', 'kleiner', 'Junge', 'spielt', 'mit', 'einem', 'roten', 'Spielzeug', '.'], ['Ein', 'Mann', 'fhrt', 'auf', 'einem', 'Skateboard', 'auf', 'einem', 'Feldweg', '.'], ['Ein', 'Mann', 'fhrt', 'auf', 'einem', 'Skateboard', 'auf', 'einem', 'Feld', '.'], ['Ein', 'Mann', 'fhrt', 'auf', 'einem', 'Fahrrad', 'durch', 'die', 'Luft', '.'], ['Ein', 'kleiner', 'Junge', 'spielt', 'mit', 'einem', 'roten', 'Spielzeug', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(de[-10:])\n",
    "print(de_preds[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Ein Mann fhrt auf einem Skateboard auf einem Feldweg . '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_MNet(transformer, 'chao ban khoe khong?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0008)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ein',\n",
       " 'Mann',\n",
       " 'fhrt',\n",
       " 'auf',\n",
       " 'einem',\n",
       " 'Fahrrad',\n",
       " 'durch',\n",
       " 'die',\n",
       " 'Luft',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_preds[-2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ee5443183715725406fd5246e657c0e511f90699501bc5e8c5d8d2b3c204bfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
