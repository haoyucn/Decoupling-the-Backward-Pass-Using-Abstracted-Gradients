{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to download a mt dataset, https://pytorch.org/tutorials/beginner/translation_transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "import torchdata\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TGT_LANGUAGE = 'de'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "# pip install -U torchdata\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download de_core_news_sm\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BATCHES = 454 # for multi30k w bs=64\n",
    "DEVICE = 'cuda:1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "class IncomingGradDiverger(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, sequentialOutput, incomingGradSaver, mSaver):\n",
    "        # print('in forward in grad saver')\n",
    "        ctx.save_for_backward(x, incomingGradSaver, sequentialOutput, mSaver)\n",
    "        ctx.mark_non_differentiable(sequentialOutput)\n",
    "        ctx.mark_non_differentiable(incomingGradSaver)\n",
    "        ctx.mark_non_differentiable(mSaver)\n",
    "        return sequentialOutput.clone().detach()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, gradients):\n",
    "        x, incomingGradSaver, sequentialOutput, mSaver = ctx.saved_tensors\n",
    "\n",
    "        incomingGradSaver.g = gradients.clone()\n",
    "        x = torch.transpose(x, 0, 1) # reshaping to put batch first\n",
    "        gradients = torch.transpose(gradients, 0, 1) # reshaping method\n",
    "        sequentialOutput = torch.transpose(sequentialOutput, 0, 1) # reshaping to put batch first\n",
    "\n",
    "        t = 1e-9\n",
    "        x_T = torch.transpose(x, -1, 1)\n",
    "        I = torch.eye(x.shape[1]).to('cuda:1')\n",
    "        pinv = torch.bmm(x_T, torch.inverse(torch.bmm(x, x_T) + t * I))\n",
    "        m = torch.bmm(pinv, sequentialOutput) # torch.transpose(pinv, -1, 1)\n",
    "        mSaver.m = m.clone() #CPU()\n",
    "\n",
    "        z = torch.bmm(gradients, m)#, None, None\n",
    "        z = torch.transpose(z, 0, 1)\n",
    "        return z, None, None, None\n",
    "\n",
    "class OutgoingGradDiverger(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, saver):\n",
    "        ctx.save_for_backward(x, saver)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gradients):\n",
    "        x, saver = ctx.saved_tensors\n",
    "        saver.g = gradients.clone()\n",
    "        return gradients, None\n",
    "\n",
    "class IncomingGradDiverger_c(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,  x, sequentialOutput, incomingGradSaver, mSaver, cNet):\n",
    "        ctx.x = x\n",
    "        ctx.sequentialOutput = sequentialOutput\n",
    "        ctx.incomingGradSaver = incomingGradSaver\n",
    "        ctx.mSaver = mSaver\n",
    "        ctx.cNet = cNet\n",
    "        return sequentialOutput.clone().detach()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, gradients):\n",
    "        x = ctx.x\n",
    "        sequentialOutput = ctx.sequentialOutput\n",
    "        incomingGradSaver = ctx.incomingGradSaver\n",
    "        mSaver = ctx.mSaver\n",
    "        cNet = ctx.cNet\n",
    "\n",
    "        incomingGradSaver.g = gradients.clone()\n",
    "        x = torch.transpose(x, 0, 1) # reshaping to put batch first\n",
    "        gradients = torch.transpose(gradients, 0, 1) # reshaping method\n",
    "        sequentialOutput = torch.transpose(sequentialOutput, 0, 1) # reshaping to put batch first    \n",
    "        t = 1e-9\n",
    "        x_T = torch.transpose(x, -1, 1)\n",
    "        I = torch.eye(x.shape[1]).to('cuda:1')\n",
    "        pinv = torch.bmm(x_T, torch.inverse(torch.bmm(x, x_T) + t * I))\n",
    "        m = torch.bmm(pinv, sequentialOutput) # torch.transpose(pinv, -1, 1)\n",
    "        mSaver.m = m.clone() #CPU()\n",
    "\n",
    "        incomingGrad_permutation = cNet(gradients)\n",
    "        incomingGrad_permutated = gradients * incomingGrad_permutation\n",
    "\n",
    "        z = torch.bmm(incomingGrad_permutated, m)#, None, None\n",
    "        z = torch.transpose(z, 0, 1)\n",
    "        return z, None, None, None, None\n",
    "    \n",
    "class CNet_Transformer(torch.nn.Module):\n",
    "    def __init__(self, sequentialLayers, output_size):\n",
    "        super(CNet_Transformer, self).__init__()\n",
    "        self.layers = []\n",
    "        for l in sequentialLayers:\n",
    "            if True: # insert all layers (attn, linear, dropout, norm)           \n",
    "                self.layers.append(l)\n",
    "        self.input_x = None\n",
    "        self.layersOutput = []\n",
    "        self.incomingGradSaver = torch.ones(1, dtype = self.layers[1].weight.dtype, requires_grad=True).to('cuda:1') # check dims of this (batch, output of M) \n",
    "        self.incomingGradDiverger = IncomingGradDiverger.apply\n",
    "        self.incomingGradDiverger_c = IncomingGradDiverger_c.apply\n",
    "        self.outgoingGradSaver = torch.ones(1, dtype = self.layers[1].weight.dtype, requires_grad=True).to('cuda:1') # check dims of this (batch, output of M) \n",
    "        self.outgoingGradDiverger = OutgoingGradDiverger.apply\n",
    "\n",
    "        self.c = torch.nn.Linear(512, 512, dtype = self.layers[1].weight.dtype).to('cuda:1')\n",
    "        \n",
    "        self.cOptimizer = torch.optim.Adam(self.c.parameters(), lr=0.01)\n",
    "        self.mSaver =  torch.ones(1, dtype = self.layers[1].weight.dtype).to('cuda:1')\n",
    "        self.useC = False\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "\n",
    "    def getLayersOutput(self, x, mask, padding_mask):\n",
    "        self.layersOutput = []\n",
    "        self.input_x = x\n",
    "\n",
    "        lo = x\n",
    "        for l in self.layers:\n",
    "            if isinstance(l, nn.MultiheadAttention):\n",
    "                if padding_mask is not None:\n",
    "                    lo = l(lo,lo,lo, attn_mask=mask, key_padding_mask=padding_mask)[0] # using attn output only not the attn output weights\n",
    "                else:\n",
    "                    lo = l(lo,lo,lo, attn_mask=mask)[0] # using attn output only not the attn output weights\n",
    "            else:\n",
    "                lo = l(lo)\n",
    "\n",
    "        return lo\n",
    "\n",
    "    def forward(self, x, mask, padding_mask):\n",
    "        x_clone = x.clone().detach()\n",
    "        x_clone.requires_grad = True\n",
    "        x_clone = self.outgoingGradDiverger(x_clone, self.outgoingGradSaver)\n",
    "        self.sequentialOutput = self.getLayersOutput(x_clone, mask, padding_mask)\n",
    "        if self.useC:\n",
    "            return self.incomingGradDiverger_c(x, self.sequentialOutput.clone().detach(), self.incomingGradSaver, self.mSaver, self.c)\n",
    "        return self.incomingGradDiverger(x, self.sequentialOutput.clone().detach(), self.incomingGradSaver, self.mSaver)\n",
    "    \n",
    "\n",
    "    def train_c(self):\n",
    "\n",
    "        incomingGrad = self.incomingGradSaver.g\n",
    "        outgoingGrad = self.outgoingGradSaver.g\n",
    "        m = self.mSaver.m\n",
    "\n",
    "        incomingGrad = torch.transpose(incomingGrad, 0, 1)\n",
    "        incomingGrad_permutation = self.c(incomingGrad)\n",
    "        incomingGrad_permutated = incomingGrad * incomingGrad_permutation\n",
    "        \n",
    "        outgoingGrad_permutated = torch.bmm(incomingGrad_permutated, m)\n",
    "        outgoingGrad_permutated = torch.transpose(outgoingGrad_permutated, 0, 1)\n",
    "\n",
    "        outgoingGrad_np = torch.bmm(incomingGrad, m)\n",
    "        outgoingGrad_np = torch.transpose(outgoingGrad_np, 0, 1)\n",
    "        loss = self.mse(outgoingGrad_permutated, outgoingGrad) + torch.dot(torch.flatten(outgoingGrad_permutated), torch.flatten(outgoingGrad_np))\n",
    "        loss.backward()\n",
    "        self.cOptimizer.step()\n",
    "\n",
    "    def backwardHidden(self):\n",
    "        self.sequentialOutput.backward(gradient = self.incomingGradSaver.g.clone().detach())\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        ps = []\n",
    "        ps.append(self.layers[0].out_proj.weight) # self attn\n",
    "        ps.append(self.layers[0].out_proj.bias)\n",
    "\n",
    "        for l in self.layers:\n",
    "            if hasattr(l, 'weight') and hasattr(l, 'bias'):\n",
    "                ps.append(l.weight)\n",
    "                ps.append(l.bias)\n",
    "        return ps\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seq2seq transformer arch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class CustomSequential(nn.Module):\n",
    "    def __init__(self, encoder_layers) -> None:\n",
    "        super(CustomSequential, self).__init__()\n",
    "        self.encoder_layers = encoder_layers\n",
    "        modules = []\n",
    "        for i in range(len(encoder_layers)):\n",
    "            modules.append(encoder_layers[i].self_attn)\n",
    "            modules.append(encoder_layers[i].linear1)\n",
    "            modules.append(encoder_layers[i].dropout)\n",
    "            modules.append(encoder_layers[i].linear2)\n",
    "            modules.append(encoder_layers[i].norm1)\n",
    "            modules.append(encoder_layers[i].norm2)\n",
    "            modules.append(encoder_layers[i].dropout)\n",
    "            modules.append(encoder_layers[i].dropout)\n",
    "            # try to add the layernorms too\n",
    "\n",
    "        self.custom_sequential = nn.Sequential(*modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer_N6(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(CustomTransformer_N6, self).__init__()\n",
    "        self.model = Transformer(d_model=d_model,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.MNet1 = CNet_Transformer(CustomSequential(self.model.encoder.layers[1:5]).custom_sequential, (self.batch_size, 512))\n",
    "        \n",
    "    def fi(self, encoder_layer, src, src_mask, src_padding_mask,):\n",
    "        # this works and is the proper way to apply self attn in the encoder and to apply the masks (i think)\n",
    "        if src_padding_mask is not None:\n",
    "            t = encoder_layer.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_padding_mask)[0] # not returning the attn output weights\n",
    "        else:\n",
    "            t = encoder_layer.self_attn(src, src, src, attn_mask=src_mask,)[0] # not returning the attn output weights\n",
    "        # print('in fi')\n",
    "        # print(len(t))\n",
    "        # print(t)\n",
    "        t = encoder_layer.linear1(t)\n",
    "        t = encoder_layer.dropout(t)\n",
    "        t = encoder_layer.linear2(t)\n",
    "        t = encoder_layer.norm1(t)\n",
    "        t = encoder_layer.norm2(t)\n",
    "        t = encoder_layer.dropout(t)\n",
    "        t = encoder_layer.dropout(t)\n",
    "        return t\n",
    "\n",
    "    def custom_encode(self, src, src_mask, src_padding_mask):\n",
    "        t = src\n",
    "        t = self.fi(self.model.encoder.layers[0], src, src_mask, src_padding_mask)\n",
    "        # t = self.model.encoder.layers[0](t, attn_mask=src_mask, key_padding_mask=src_padding_mask)\n",
    "        # print('here', t.shape)\n",
    "        # print(t)\n",
    "        t = self.MNet1(t, src_mask, src_padding_mask)\n",
    "        # t = self.fi(self.model.encoder.layers[-2], t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[-1], src, src_mask, src_padding_mask)\n",
    "\n",
    "        return t\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        # mem = self.model.encoder(src) # apply M\n",
    "        t = src\n",
    "        t = self.fi(self.model.encoder.layers[0], src, src_mask, src_padding_mask)\n",
    "        # t = self.model.encoder.layers[0](t, attn_mask=src_mask, key_padding_mask=src_padding_mask)\n",
    "        # print('here', t.shape)\n",
    "        # print(t)\n",
    "        t = self.MNet1(t, src_mask, src_padding_mask)\n",
    "        # t = self.fi(self.model.encoder.layers[5], t, src_mask, src_padding_mask)\n",
    "        # t = self.MNet2(t, src_mask, src_padding_mask)\n",
    "        # print('forward in custom transformer, ', t.requires_grad)\n",
    "        # t = self.fi(self.model.encoder.layers[-2], t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[-1], t, src_mask, src_padding_mask)\n",
    "\n",
    "\n",
    "\n",
    "        mem = t #(in this case just the src padding mask which is boolean)\n",
    "        output = self.model.decoder(tgt, mem, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        ps = []\n",
    "        mnet1Ps = self.MNet1.get_parameters()\n",
    "        # mnet2Ps = self.MNet2.get_parameters()\n",
    "        \n",
    "\n",
    "        # add params outside of layers abstracted by Ms in encoder and decoder\n",
    "        kept_encoder_layers = [self.model.encoder.layers[0], self.model.encoder.layers[-1]]\n",
    "        for kept_layer in kept_encoder_layers:\n",
    "            ps.append(kept_layer.self_attn.out_proj.weight)\n",
    "            ps.append(kept_layer.self_attn.out_proj.bias)\n",
    "            ps.append(kept_layer.linear1.weight)\n",
    "            ps.append(kept_layer.linear1.bias)\n",
    "            ps.append(kept_layer.linear2.weight)\n",
    "            ps.append(kept_layer.linear2.bias)\n",
    "\n",
    "        for block in self.model.decoder.layers:\n",
    "            ps.append(block.self_attn.out_proj.weight)\n",
    "            ps.append(block.self_attn.out_proj.bias)\n",
    "            ps.append(block.multihead_attn.out_proj.weight)\n",
    "            ps.append(block.multihead_attn.out_proj.bias)\n",
    "            ps.append(block.linear1.weight)\n",
    "            ps.append(block.linear1.bias)\n",
    "            ps.append(block.linear2.weight)\n",
    "            ps.append(block.linear2.bias)\n",
    "            \n",
    "\n",
    "        for p in mnet1Ps:\n",
    "            ps.append(p)\n",
    "        # for p in mnet2Ps:\n",
    "        #     ps.append(p)\n",
    "        return ps\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer_N6(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer_N6, self).__init__()\n",
    "        self.transformer = CustomTransformer_N6(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size) # not returning the attn output weights\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        # print('src_emb.shape ', src_emb.shape)\n",
    "        # print('src_mask.shape ', src_mask.shape)\n",
    "        # print('src_emb ', src_emb)\n",
    "        # print('src_mask ', src_mask)\n",
    "        # exit(0)\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask) # find out how masking is applied within forward method\n",
    "        return self.generator(outs)\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        ps = []\n",
    "        transformer_ps = self.transformer.get_parameters()\n",
    "        ps.extend(transformer_ps)\n",
    "        # ps.append(self.generator.weight)\n",
    "        # ps.append(self.generator.bias)\n",
    "        # ps.append(self.src_tok_emb.embedding.weight)\n",
    "        # ps.append(self.tgt_tok_emb.embedding.weight)\n",
    "\n",
    "        return ps\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer_N12(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(CustomTransformer_N12, self).__init__()\n",
    "        self.model = Transformer(d_model=d_model,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.MNet1 = CNet_Transformer(CustomSequential(self.model.encoder.layers[1:5]).custom_sequential, (self.batch_size, 512))\n",
    "        self.MNet2 = CNet_Transformer(CustomSequential(self.model.encoder.layers[6:-2]).custom_sequential, (self.batch_size, 512))\n",
    "\n",
    "    def fi(self, encoder_layer, src, src_mask, src_padding_mask,):\n",
    "        # this works and is the proper way to apply self attn in the encoder and to apply the masks (i think)\n",
    "        if src_padding_mask is not None:\n",
    "            t = encoder_layer.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_padding_mask)[0] # not returning the attn output weights\n",
    "        else:\n",
    "            t = encoder_layer.self_attn(src, src, src, attn_mask=src_mask,)[0] # not returning the attn output weights\n",
    "        t = encoder_layer.linear1(t)\n",
    "        t = encoder_layer.dropout(t)\n",
    "        t = encoder_layer.linear2(t)\n",
    "        t = encoder_layer.norm1(t)\n",
    "        t = encoder_layer.norm2(t)\n",
    "        t = encoder_layer.dropout(t)\n",
    "        t = encoder_layer.dropout(t)\n",
    "        return t\n",
    "\n",
    "    def custom_encode(self, src, src_mask, src_padding_mask):\n",
    "        t = src\n",
    "        t = self.fi(self.model.encoder.layers[0], src, src_mask, src_padding_mask)\n",
    "        t = self.MNet1(t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[5], src, src_mask, src_padding_mask)\n",
    "        t = self.MNet2(t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[-2], t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[-1], src, src_mask, src_padding_mask)\n",
    "\n",
    "        return t\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        t = src\n",
    "        t = self.fi(self.model.encoder.layers[0], src, src_mask, src_padding_mask)\n",
    "        t = self.MNet1(t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[5], t, src_mask, src_padding_mask)\n",
    "        t = self.MNet2(t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[-2], t, src_mask, src_padding_mask)\n",
    "        t = self.fi(self.model.encoder.layers[-1], t, src_mask, src_padding_mask)\n",
    "\n",
    "        mem = t #(in this case just the src padding mask which is boolean)\n",
    "        output = self.model.decoder(tgt, mem, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        ps = []\n",
    "        mnet1Ps = self.MNet1.get_parameters()\n",
    "        mnet2Ps = self.MNet2.get_parameters()\n",
    "\n",
    "        # add params outside of layers abstracted by Ms in encoder and decoder\n",
    "        kept_encoder_layers = [self.model.encoder.layers[0], self.model.encoder.layers[5], self.model.encoder.layers[-1]]\n",
    "        for kept_layer in kept_encoder_layers:\n",
    "            ps.append(kept_layer.self_attn.out_proj.weight)\n",
    "            ps.append(kept_layer.self_attn.out_proj.bias)\n",
    "            ps.append(kept_layer.linear1.weight)\n",
    "            ps.append(kept_layer.linear1.bias)\n",
    "            ps.append(kept_layer.linear2.weight)\n",
    "            ps.append(kept_layer.linear2.bias)\n",
    "\n",
    "        for block in self.model.decoder.layers:\n",
    "            ps.append(block.self_attn.out_proj.weight)\n",
    "            ps.append(block.self_attn.out_proj.bias)\n",
    "            ps.append(block.multihead_attn.out_proj.weight)\n",
    "            ps.append(block.multihead_attn.out_proj.bias)\n",
    "            ps.append(block.linear1.weight)\n",
    "            ps.append(block.linear1.bias)\n",
    "            ps.append(block.linear2.weight)\n",
    "            ps.append(block.linear2.bias)\n",
    "\n",
    "        for p in mnet1Ps:\n",
    "            ps.append(p)\n",
    "        for p in mnet2Ps:\n",
    "            ps.append(p)\n",
    "        return ps\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer_N12(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer_N12, self).__init__()\n",
    "        self.transformer = CustomTransformer_N12(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size) # not returning the attn output weights\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask) # find out how masking is applied within forward method\n",
    "        return self.generator(outs)\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        ps = []\n",
    "        transformer_ps = self.transformer.get_parameters()\n",
    "        ps += transformer_ps\n",
    "        ps.append(self.generator.weight)\n",
    "        ps.append(self.generator.bias)\n",
    "        ps.append(self.src_tok_emb.embedding.weight)\n",
    "        ps.append(self.tgt_tok_emb.embedding.weight)\n",
    "\n",
    "        return ps\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 2048\n",
    "BATCH_SIZE = 64\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "\n",
    "transformer = Seq2SeqTransformer_N6(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to('cuda:1')\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX) \n",
    "lr = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "print(len(transformer.get_parameters()))\n",
    "print(sum(1 for _ in transformer.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "all_params = []\n",
    "for n,p in transformer.named_parameters():\n",
    "    if 'encoder.layers' in n:\n",
    "        if 'encoder.layers.0' in n or 'encoder.layers.5' in n: # for N_enconder=6\n",
    "            all_params.append(p)\n",
    "    else:\n",
    "        all_params.append(p)\n",
    "    # print(n)\n",
    "    ct += 1\n",
    "len(all_params)\n",
    "all_params.extend(transformer.get_parameters())\n",
    "len(all_params)\n",
    "\n",
    "all_params = [*set(all_params)]\n",
    "print(len(all_params))\n",
    "optimizer = torch.optim.Adam(all_params, lr=lr, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_batches():\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    ct = 0\n",
    "    for _ in enumerate(train_dataloader):\n",
    "        ct +=1 \n",
    "    return ct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_BATCH_SIZE = 13 # for bs=64 and on multi30k (using attn is all you need setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from threading import Thread\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def update_lr(step_num):\n",
    "    num_warmup = 4000\n",
    "    return (EMB_SIZE)**-0.5 * min(step_num**(-0.5), step_num * num_warmup**(-1.5))\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    saved_losses = []\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    step = 0\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        # if src.shape[1] != BATCH_SIZE:\n",
    "        #     print('not skipping batch of size ',src.shape[1])\n",
    "        #     # continue \n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        step_num = epoch * step\n",
    "        if epoch > 1 and step == 0:\n",
    "            step_num = epoch * NUM_BATCHES\n",
    "        optim.param_groups[0]['lr'] = update_lr(step_num)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        model.transformer.MNet1.backwardHidden()\n",
    "        model.transformer.MNet2.backwardHidden()\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        saved_losses.append(loss.item())\n",
    "        step += 1\n",
    "\n",
    "    return losses / len(list(train_dataloader)), saved_losses\n",
    "\n",
    "def update1(network):\n",
    "  network.MNet1.backwardHidden() \n",
    "\n",
    "def update2(network):\n",
    "  network.MNet2.backwardHidden() \n",
    "\n",
    "def train_epoch_parallel(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    saved_losses = []\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    step = 1\n",
    "    minibatch_counter = 0\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        step_num = int((epoch-1) * (NUM_BATCHES // LARGE_BATCH_SIZE)) + step       \n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        p1 = Thread(target=update1, args=[model.transformer]) # start two independent threads\n",
    "        # p2 = Thread(target=update2, args=[model.transformer]) # start two independent threads\n",
    "        \n",
    "        p1.start()\n",
    "        # p2.start()\n",
    "        if hasattr(model.transformer, 'MNet2'): # in the case n = 6 there is no mnet2\n",
    "            model.transformer.MNet2.backwardHidden()\n",
    "\n",
    "\n",
    "        p1.join()\n",
    "        model.transformer.MNet1.train_c() \n",
    "        if hasattr(model.transformer, 'MNet2'): # in the case n = 6 there is no mnet2\n",
    "            model.transformer.MNet2.train_c()\n",
    "\n",
    "        \n",
    "        losses += loss.item()\n",
    "        saved_losses.append(loss.item())\n",
    "        minibatch_counter += 1\n",
    "        \n",
    "        if minibatch_counter > LARGE_BATCH_SIZE - 1: \n",
    "            minibatch_counter = 0\n",
    "            optimizer.param_groups[0]['lr'] = update_lr(step_num)\n",
    "            optimizer.step()\n",
    "            # reset grad\n",
    "            optimizer.zero_grad()\n",
    "            step += 1\n",
    "\n",
    "    # update lr and weights as long as they weren't JUST updated\n",
    "    if minibatch_counter > 0:\n",
    "        # update the lr and the weights\n",
    "        optimizer.param_groups[0]['lr'] = update_lr(step_num)\n",
    "        optimizer.step()\n",
    "        # reset grad\n",
    "        optimizer.zero_grad()\n",
    "        step += 1\n",
    "\n",
    "    return losses / len(list(train_dataloader)), saved_losses\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    saved_losses_val = []\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "        saved_losses_val.append(loss.item())\n",
    "\n",
    "    return losses / len(list(val_dataloader)), saved_losses_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps:  27240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/miniconda/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/hao/miniconda/lib/python3.8/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 9.606, Val loss: 9.299, Epoch time = 69.766s\n",
      "Epoch: 2, Train loss: 9.174, Val loss: 8.948, Epoch time = 69.988s\n",
      "Epoch: 3, Train loss: 8.809, Val loss: 8.519, Epoch time = 70.171s\n",
      "Epoch: 4, Train loss: 8.284, Val loss: 7.871, Epoch time = 71.131s\n",
      "Epoch: 5, Train loss: 7.594, Val loss: 7.208, Epoch time = 70.100s\n",
      "Epoch: 6, Train loss: 6.943, Val loss: 6.589, Epoch time = 66.682s\n",
      "Epoch: 7, Train loss: 6.336, Val loss: 6.040, Epoch time = 70.196s\n",
      "Epoch: 8, Train loss: 5.811, Val loss: 5.591, Epoch time = 69.645s\n",
      "Epoch: 9, Train loss: 5.413, Val loss: 5.294, Epoch time = 69.971s\n",
      "Epoch: 10, Train loss: 5.129, Val loss: 5.044, Epoch time = 71.568s\n",
      "Epoch: 11, Train loss: 4.878, Val loss: 4.804, Epoch time = 70.228s\n",
      "Epoch: 12, Train loss: 4.635, Val loss: 4.579, Epoch time = 70.714s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb Cell 26\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# raise Exception\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     transformer\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(PATH))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTransformer model weights loaded\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m     \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seq2seq_transformer_multi30k_weights_m_tmp.pt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb Cell 26\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         transformer\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mMNet2\u001b[39m.\u001b[39museC \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m start_time \u001b[39m=\u001b[39m timer()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m train_loss, saved_losses_train \u001b[39m=\u001b[39m train_epoch_parallel(transformer, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# train_loss, saved_losses_train = train_epoch(transformer, optimizer)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m end_time \u001b[39m=\u001b[39m timer()\n",
      "\u001b[1;32m/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb Cell 26\u001b[0m in \u001b[0;36mtrain_epoch_parallel\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m step_num \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m((epoch\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m (NUM_BATCHES \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m LARGE_BATCH_SIZE)) \u001b[39m+\u001b[39m step       \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m tgt_out \u001b[39m=\u001b[39m tgt[\u001b[39m1\u001b[39m:, :]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, logits\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), tgt_out\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X42sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m p1 \u001b[39m=\u001b[39m Thread(target\u001b[39m=\u001b[39mupdate1, args\u001b[39m=\u001b[39m[model\u001b[39m.\u001b[39mtransformer]) \u001b[39m# start two independent threads\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "NUM_EPOCHS = 60  # quarter of way training according to attn paper (looks like it starts overfitting pretty early)\n",
    "DEVICE = 'cuda:1'\n",
    "# PATH = 'seq2seq_transformer_multi30k_weights.pt'\n",
    "PATH = 'seq2seq_transformer_multi30k_weights_m_tmp.pt'\n",
    "NUM_BATCHES = get_num_batches() \n",
    "NUM_STEPS = NUM_EPOCHS * NUM_BATCHES\n",
    "print('Num steps: ', NUM_STEPS)\n",
    "\n",
    "try:\n",
    "    # raise Exception\n",
    "    transformer.load_state_dict(torch.load(PATH))\n",
    "    print('Transformer model weights loaded')\n",
    "except Exception:\n",
    "    # early_stopper = EarlyStopper(patience=3, min_delta=0.025)\n",
    "    t0 = time.time()\n",
    "    training_losses_to_plot = []\n",
    "    val_losses_to_plot = []\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        if epoch >= 0:\n",
    "            transformer.transformer.MNet1.useC = True\n",
    "            if hasattr(transformer.transformer, 'MNet2'): # in the case n = 6 there is no mnet2\n",
    "                transformer.transformer.MNet2.useC = True\n",
    "        start_time = timer()\n",
    "        train_loss, saved_losses_train = train_epoch_parallel(transformer, optimizer)\n",
    "        # train_loss, saved_losses_train = train_epoch(transformer, optimizer)\n",
    "        end_time = timer()\n",
    "        val_loss, saved_losses_val = evaluate(transformer)\n",
    "        # print('M computation TIME PER EPOCH: ', lst_sqs_sum)\n",
    "        print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "        lst_sqs_sum = 0\n",
    "        if epoch > 25:\n",
    "            torch.save(transformer.state_dict(), f'tmp_transformer_base_w_MNet_weights_epoch={epoch}.pt')\n",
    "\n",
    "\n",
    "        if len(val_losses_to_plot) > 0 and val_loss < min(val_losses_to_plot):\n",
    "            best_model = transformer\n",
    "            torch.save(best_model.state_dict(), 'tmp_transformer_base_w_MNet_weights.pt')\n",
    "            # keep the model with lowest val loss\n",
    "        \n",
    "        \n",
    "        training_losses_to_plot.append(train_loss)\n",
    "        val_losses_to_plot.append(val_loss)\n",
    "\n",
    "\n",
    "        # if early_stopper.early_stop(val_loss):\n",
    "        #     print(\"<EARLY STOP> model done training.\")             \n",
    "        #     break\n",
    "    tf = time.time()\n",
    "    PATH = f'seq2seq_transformer_multi30k_weights_m_tmp_epochs={epoch}.pt'\n",
    "    torch.save(transformer.state_dict(), PATH)\n",
    "    print(f'Transformer model saved ({PATH})')\n",
    "    print(f'Trained for {NUM_EPOCHS} epochs in {tf - t0} seconds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve training further, can use adaptive learning rate schedule as described in sec 5.3 and also label smoothing as described in sec 5.4 - done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = best_model\n",
    "torch.save(transformer.state_dict(), 'CNet_transformer_best_val_loss_tmp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num steps:  27240\n"
     ]
    }
   ],
   "source": [
    "print(\"total num steps: \", NUM_EPOCHS * NUM_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_state_dict(torch.load('CNet_transformer_best_val_loss_tmp.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plotting curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (60,) and (49,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb Cell 32\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m s \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(training_losses_to_plot) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(val_losses_to_plot)) \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# plt.plot(np.arange(0, len(training_losses_to_plot), 10), training_losses_to_plot[::10])\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39;49mplot(\u001b[39mrange\u001b[39;49m(NUM_EPOCHS), training_losses_to_plot, label\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTraining\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X51sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(NUM_EPOCHS), val_losses_to_plot, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m plt\u001b[39m.\u001b[39mlegend()\n",
      "File \u001b[0;32m~/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py:3019\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   3018\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 3019\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   3020\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   3021\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.8/site-packages/matplotlib/axes/_axes.py:1605\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1603\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1604\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1605\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1606\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1607\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.8/site-packages/matplotlib/axes/_base.py:315\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    314\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 315\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(this, kwargs)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.8/site-packages/matplotlib/axes/_base.py:501\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    500\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (60,) and (49,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "s = (len(training_losses_to_plot) // len(val_losses_to_plot)) \n",
    "\n",
    "# plt.plot(np.arange(0, len(training_losses_to_plot), 10), training_losses_to_plot[::10])\n",
    "plt.plot(range(NUM_EPOCHS), training_losses_to_plot, label='Training')\n",
    "plt.plot(range(NUM_EPOCHS), val_losses_to_plot, label='Validation')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchmetrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb Cell 34\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m bleu_score\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# function to generate output sequence using greedy algorithm\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hao/Documents/magic-m/seq2seq_transformer_w_mnet_v4_cnet.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgreedy_decode\u001b[39m(model, src, src_mask, max_len, start_symbol):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchmetrics'"
     ]
    }
   ],
   "source": [
    "from torchmetrics.functional import bleu_score\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encoder(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode_MNet(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "    # print(src.shape)\n",
    "    src_emb = model.positional_encoding(model.src_tok_emb(src))\n",
    "    # print(src_emb.shape)\n",
    "    memory = model.transformer.custom_encode(src_emb, src_mask, None)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        ys_emb = model.positional_encoding(model.tgt_tok_emb(ys))\n",
    "        out = model.transformer.model.decoder(ys_emb, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate_MNet(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    # print('in translate, ', src.shape)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode_MNet(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "def test(model,):\n",
    "    test_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    \n",
    "    # compute the avg bleu score\n",
    "    bleu = 0\n",
    "    ct = 0\n",
    "    for data_sample in test_iter:\n",
    "        ct += 1\n",
    "        # print(data_sample)\n",
    "        src = data_sample[0]\n",
    "        tgt = data_sample[1]\n",
    "        pred = translate_MNet(model, src)\n",
    "        bleu += bleu_score([pred], [tgt])\n",
    "    return bleu / ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_MNet(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "    # print(src.shape)\n",
    "    src_emb = model.positional_encoding(model.src_tok_emb(src))\n",
    "    # print(src_emb.shape)\n",
    "    memory = model.transformer.custom_encode(src_emb, src_mask, None)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        ys_emb = model.positional_encoding(model.tgt_tok_emb(ys))\n",
    "        out = model.transformer.model.decoder(ys_emb, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "def translate_MNet(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    # print('in translate, ', src.shape)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode_MNet(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/.local/lib/python3.8/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/hao/.local/lib/python3.8/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "# transformer.load_state_dict(torch.load('seq2seq_transformer_multi30k_weights_epochs=21.pt'))\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "test_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "de = []\n",
    "de_refs= []\n",
    "en = []\n",
    "de_preds = []\n",
    "# de_preds2 = []\n",
    "sentence_scores = []\n",
    "for data_sample in test_iter:\n",
    "    if len(data_sample[0]) < 3 or len(data_sample[1]) < 3:\n",
    "        print('here')\n",
    "        continue\n",
    "    \n",
    "    en.append(data_sample[0].split())\n",
    "    de.append(data_sample[1].split())\n",
    "    de_refs.append([data_sample[1].split()])\n",
    "    \n",
    "    pred = translate_MNet(transformer, data_sample[0])\n",
    "    pred_split = pred.split()\n",
    "    # if pred_split[-1] == '.':\n",
    "    #     pred_split = pred_split[:-1] # trim periods\n",
    "    de_preds.append(pred_split)\n",
    "    sentence_scores.append(sentence_bleu(de[-1], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005378602599196523"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu(de_refs, de_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg BLEU EN->DE scores (with MNet): \n",
      "0.10007249202070026\n"
     ]
    }
   ],
   "source": [
    "print('avg BLEU EN->DE scores (with MNet): ')\n",
    "\n",
    "print(sum(sentence_scores) / len(sentence_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Eine', 'brnette', 'Frau', 'steht', 'auf', 'dem', 'Gehweg', 'und', 'blickt', 'die', 'Strae', 'hinunter.'], ['Eine', 'Gruppe', 'von', 'drei', 'Freunden', 'unterhlt', 'sich', 'in', 'einem', 'Haus.'], ['Zwei', 'Chinesen', 'stehen', 'an', 'einer', 'Wandtafel.'], ['Eine', 'Person', 'in', 'Bluejeans', 'und', 'rotem', 'Pullover', 'biegt', 'um', 'die', 'Ecke', 'einer', 'Ziegelmauer.'], ['Bauern', 'betreiben', 'tagsber', 'ihre', 'Landwirtschaft.'], ['Bei', 'einer', 'Art', 'Jahrmarkt', 'stellt', 'ein', 'Mann', 'Zuckerwatte', 'her.'], ['Eine', 'Gruppe', 'von', 'Polizisten', 'steht', 'vor', 'einem', 'Bus.'], ['Eine', 'ltere', 'weihaarige', 'Frau', 'sieht', 'in', 'ihre', 'Kasse', 'und', 'blickt', 'durch', 'ihre', 'Brillenglser', 'hindurch.'], ['Zwei', 'Mnner', 'stehen', 'an', 'Telefonzellen', 'im', 'Freien.'], ['Zwei', 'Frauen', 'in', 'Rot', 'und', 'ein', 'Mann,', 'der', 'aus', 'einer', 'transportablen', 'Toilette', 'kommt.']]\n",
      "[['Eine', 'Frau', 'mit', 'einem', 'weien', 'Oberteil', 'und', 'einem', 'schwarzen', 'Rock', 'geht', 'spazieren', '.'], ['Eine', 'Frau', 'mit', 'einem', 'schwarzen', 'Hund', 'luft', 'durch', 'den', 'Schnee', '.'], ['Zwei', 'Frauen', 'in', 'schwarzen', 'Hemden', 'stehen', 'vor', 'einem', 'Gebude', '.'], ['Eine', 'Frau', 'mit', 'einem', 'weien', 'Oberteil', 'steht', 'vor', 'einem', 'Gebude', '.'], ['Eine', 'Person', 'mit', 'einem', 'roten', 'Helm', 'fhrt', 'Ski', '.'], ['Eine', 'Frau', 'mit', 'einem', 'weien', 'Hut', 'geht', 'an', 'einem', 'Gebude', 'vorbei', '.'], ['Eine', 'Frau', 'mit', 'einem', 'Hund', 'luft', 'durch', 'den', 'Schnee', '.'], ['Eine', 'Frau', 'mit', 'einem', 'weien', 'Oberteil', 'steht', 'vor', 'einem', 'Gebude', '.'], ['Eine', 'Frau', 'mit', 'einem', 'schwarzen', 'Hund', 'luft', 'durch', 'den', 'Schnee', '.'], ['Eine', 'Frau', 'mit', 'einem', 'weien', 'Oberteil', 'und', 'einem', 'schwarzen', 'Rock', 'geht', 'spazieren', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(de[-10:])\n",
    "print(de_preds[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_MNet(transformer, 'chao ban khoe khong?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_preds[-2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ee5443183715725406fd5246e657c0e511f90699501bc5e8c5d8d2b3c204bfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
