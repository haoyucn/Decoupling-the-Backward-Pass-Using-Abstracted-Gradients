{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a429510",
   "metadata": {
    "id": "SX7UC-8jTsp7",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "\n",
    "<center><h1>Speeding up the Backward Pass</h1> </center>\n",
    "\n",
    "\n",
    "<center>\n",
    "<p>Seq2Seq Testing - IWSLT2017</p>\n",
    "</center>\n",
    "\n",
    "[Trnasformer Implementation code from ](https://nlp.seas.harvard.edu/2018/04/03/attention.html):\n",
    "   [Sasha Rush](http://rush-nlp.com/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7fa96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:17.544078Z",
     "iopub.status.busy": "2022-05-02T01:25:17.542981Z",
     "iopub.status.idle": "2022-05-02T01:25:17.545515Z",
     "shell.execute_reply": "2022-05-02T01:25:17.546285Z"
    },
    "id": "NwClcbH6Tsp8",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!pip install GPUtil\n",
    "!pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n",
    "!python3 -m spacy download de_core_news_sm\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf3deb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:17.560273Z",
     "iopub.status.busy": "2022-05-02T01:25:17.559273Z",
     "iopub.status.idle": "2022-05-02T01:25:18.690005Z",
     "shell.execute_reply": "2022-05-02T01:25:18.690769Z"
    },
    "id": "v1-1MX6oTsp9",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import gc\n",
    "\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be5fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60359a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.702574Z",
     "iopub.status.busy": "2022-05-02T01:25:18.701680Z",
     "iopub.status.idle": "2022-05-02T01:25:18.704131Z",
     "shell.execute_reply": "2022-05-02T01:25:18.704839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some convenience helper functions used throughout the notebook\n",
    "\n",
    "\n",
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e7e57e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.716980Z",
     "iopub.status.busy": "2022-05-02T01:25:18.716023Z",
     "iopub.status.idle": "2022-05-02T01:25:18.718971Z",
     "shell.execute_reply": "2022-05-02T01:25:18.718190Z"
    },
    "id": "k0XGXhzRTsqB"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many\n",
    "    other models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bfcee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.725923Z",
     "iopub.status.busy": "2022-05-02T01:25:18.724967Z",
     "iopub.status.idle": "2022-05-02T01:25:18.727616Z",
     "shell.execute_reply": "2022-05-02T01:25:18.726912Z"
    },
    "id": "NKGoH2RsTsqC",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "380da5d0",
   "metadata": {},
   "source": [
    "## MNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import math\n",
    "m_shape = (512, 512) # EMB, EMB\n",
    "\n",
    "class GradSaver_Compute_M(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, sequentialOutput, saver):\n",
    "        # print('in forward in grad saver')\n",
    "        ctx.save_for_backward(x, saver, sequentialOutput)\n",
    "        return sequentialOutput.clone().detach()\n",
    "    \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gradients):\n",
    "        global lst_sqs_sum\n",
    "        \n",
    "        x, saver, s = ctx.saved_tensors\n",
    "        g = gradients\n",
    "        saver.g = gradients.clone()\n",
    "\n",
    "        l,b,d = x.shape\n",
    "\n",
    "        # reshape\n",
    "        x = x.reshape(l*b, d)\n",
    "        s = s.reshape(l*b, d)\n",
    "        g = g.reshape(l*b, d)\n",
    "\n",
    "        K = math.ceil(l*b / d)\n",
    "        pad_size = ( (K*d) - (b*l) ) # should be int\n",
    "        pad_value = 0\n",
    "\n",
    "        # now apply padding to x,s,g and reshape\n",
    "        x_pad = torch.nn.functional.pad(x, (0,0,0,pad_size), 'constant', pad_value).reshape(K,d,d)\n",
    "        s_pad = torch.nn.functional.pad(s, (0,0,0,pad_size), 'constant', pad_value).reshape(K,d,d)\n",
    "        g_pad = torch.nn.functional.pad(g, (0,0,0,pad_size), 'constant', pad_value).reshape(K,d,d)\n",
    "\n",
    "        delta = 1e-9\n",
    "        perturbation = delta + torch.eye(d).to('cuda:1')\n",
    "        saver.g = gradients.clone()\n",
    "\n",
    "        gbar = []\n",
    "        gbar = [torch.mm(g_pad[k,:,:], layerwisenorm_M(softmax_func_M(torch.linalg.solve(x_pad[k,:,:] + perturbation, s_pad[k,:,:])))) for k in range(K)]\n",
    "\n",
    "        g_out = torch.cat(gbar)\n",
    "\n",
    "        # unpad and reshape\n",
    "        g_out = g_out[:g_out.shape[0]-pad_size,:].reshape(l,b,d)        \n",
    "        return g_out, None, None\n",
    "\n",
    "class GradSaver_As_Is(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, sequentialOutput, saver):\n",
    "        ctx.save_for_backward(x, saver, sequentialOutput)\n",
    "        return sequentialOutput.clone().detach()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gradients):\n",
    "        global lst_sqs_sum\n",
    "        \n",
    "        x, saver, s = ctx.saved_tensors\n",
    "        saver.g = gradients.clone()\n",
    "        return gradients, None, None\n",
    "    \n",
    "\n",
    "class MNet_Transformer_As_Is(torch.nn.Module):\n",
    "    def __init__(self, sequentialLayers, output_size):\n",
    "        print('Creating MNet_Transformer')\n",
    "        super(MNet_Transformer_As_Is, self).__init__()\n",
    "        self.layers = []\n",
    "        for l in sequentialLayers:\n",
    "            self.layers.append(l)\n",
    "        self.input_x = None\n",
    "        self.layersOutput = []\n",
    "        self.saver = torch.ones(output_size, dtype = self.layers[0].norm.a_2.data.dtype, requires_grad=True).to('cuda:1') # check dims of this (batch, output of M) \n",
    "        self.gradDiverge = GradSaver_As_Is.apply\n",
    "        self.sequentialOutput = None\n",
    "\n",
    "\n",
    "        # vars for custom_attn\n",
    "        self.h = H\n",
    "        self.d_k = MODEL_DIM // self.h\n",
    "        self.linears = clones(nn.Linear(MODEL_DIM, MODEL_DIM), 4)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.self_attn_func = MultiHeadedAttention(H, MODEL_DIM)\n",
    "\n",
    "        self.use_M = True\n",
    "\n",
    "    def getLayersOutput(self, x, mask,):\n",
    "        self.layersOutput = []\n",
    "\n",
    "        if self.use_M:\n",
    "            x_clone = x.clone().detach()\n",
    "            x_clone.requires_grad = False\n",
    "            self.input_x = x_clone\n",
    "        else:\n",
    "            self.input_x = x\n",
    "\n",
    "        sublayer_ct = 0\n",
    "        lo = self.input_x\n",
    "        for l in self.layers:\n",
    "\n",
    "            if 'sublayer' in str(type(l)).lower() and sublayer_ct == 0:\n",
    "                lo = l(lo, lambda x: self.self_attn_func(x, x, x, mask))\n",
    "                self.attn = l.attn_mat\n",
    "                sublayer_ct += 1\n",
    "            elif 'sublayer' in str(type(l)).lower() and sublayer_ct > 0:\n",
    "                lo = l(lo, feed_forward_layer)\n",
    "                sublayer_ct = 0\n",
    "            elif 'feed' in str(type(l)).lower():\n",
    "                feed_forward_layer = l # save for second sublayer\n",
    "            else:\n",
    "                print('ERROR: unexpected layer: ', l)\n",
    "\n",
    "        return lo\n",
    "\n",
    "    def forward(self, x, mask,):\n",
    "        if self.use_M:\n",
    "            x_clone = x.clone().detach()\n",
    "            x_clone.requires_grad = False\n",
    "            self.sequentialOutput = self.getLayersOutput(x_clone, mask,)\n",
    "            if self.saver.shape != self.sequentialOutput.shape:\n",
    "                self.saver = torch.ones(self.sequentialOutput.shape).to('cuda:1')\n",
    "\n",
    "            t = self.gradDiverge(x, self.sequentialOutput.clone().detach(), self.saver)\n",
    "        else:\n",
    "            self.sequentialOutput = self.getLayersOutput(x, mask,)\n",
    "            t = self.sequentialOutput\n",
    "        return t\n",
    "\n",
    "    def backwardHidden(self):\n",
    "        self.sequentialOutput.backward(gradient = self.saver.g.clone().detach())\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        ps = []\n",
    "        ps.append(self.layers[0].out_proj.weight) # self attn\n",
    "        ps.append(self.layers[0].out_proj.bias)\n",
    "\n",
    "        for l in self.layers:\n",
    "            if hasattr(l, 'weight') and hasattr(l, 'bias'):\n",
    "                ps.append(l.weight)\n",
    "                ps.append(l.bias)\n",
    "        return ps\n",
    "    \n",
    "class MNet_Transformer_Compute_M(torch.nn.Module):\n",
    "    def __init__(self, sequentialLayers, output_size):\n",
    "        print('Creating MNet_Transformer')\n",
    "        super(MNet_Transformer_Compute_M, self).__init__()\n",
    "        self.layers = []\n",
    "        for l in sequentialLayers:\n",
    "            self.layers.append(l)\n",
    "        self.input_x = None\n",
    "        self.layersOutput = []\n",
    "        self.saver = torch.ones(output_size, dtype = self.layers[0].norm.a_2.data.dtype, requires_grad=True).to('cuda:1') # check dims of this (batch, output of M) \n",
    "        self.gradDiverge = GradSaver_Compute_M.apply\n",
    "        self.sequentialOutput = None\n",
    "\n",
    "\n",
    "        # vars for custom_attn\n",
    "        self.h = H\n",
    "        self.d_k = MODEL_DIM // self.h\n",
    "        self.linears = clones(nn.Linear(MODEL_DIM, MODEL_DIM), 4)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.self_attn_func = MultiHeadedAttention(H, MODEL_DIM)\n",
    "        self.use_M = True\n",
    "\n",
    "    def getLayersOutput(self, x, mask,):\n",
    "        self.layersOutput = []\n",
    "\n",
    "        if self.use_M:\n",
    "            x_clone = x.clone().detach()\n",
    "            x_clone.requires_grad = False\n",
    "            self.input_x = x_clone\n",
    "        else:\n",
    "            self.input_x = x\n",
    "\n",
    "        sublayer_ct = 0\n",
    "        lo = self.input_x\n",
    "        for l in self.layers:\n",
    "\n",
    "            if 'sublayer' in str(type(l)).lower() and sublayer_ct == 0:\n",
    "                lo = l(lo, lambda x: self.self_attn_func(x, x, x, mask))\n",
    "                self.attn = l.attn_mat\n",
    "                sublayer_ct += 1\n",
    "            elif 'sublayer' in str(type(l)).lower() and sublayer_ct > 0:\n",
    "                lo = l(lo, feed_forward_layer)\n",
    "                sublayer_ct = 0\n",
    "            elif 'feed' in str(type(l)).lower():\n",
    "                feed_forward_layer = l # save for second sublayer\n",
    "            else:\n",
    "                print('ERROR: unexpected layer: ', l)\n",
    "\n",
    "        return lo\n",
    "\n",
    "    def forward(self, x, mask,):\n",
    "        if self.use_M:\n",
    "            x_clone = x.clone().detach()\n",
    "            x_clone.requires_grad = False\n",
    "            self.sequentialOutput = self.getLayersOutput(x_clone, mask,)\n",
    "            if self.saver.shape != self.sequentialOutput.shape:\n",
    "                self.saver = torch.ones(self.sequentialOutput.shape).to('cuda:1')\n",
    "\n",
    "            t = self.gradDiverge(x, self.sequentialOutput.clone().detach(), self.saver)\n",
    "        else:\n",
    "            self.sequentialOutput = self.getLayersOutput(x, mask,)\n",
    "            t = self.sequentialOutput\n",
    "        return t\n",
    "\n",
    "    def backwardHidden(self):\n",
    "        self.sequentialOutput.backward(gradient = self.saver.g.clone().detach())\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        ps = []\n",
    "        ps.append(self.layers[0].out_proj.weight) # self attn\n",
    "        ps.append(self.layers[0].out_proj.bias)\n",
    "\n",
    "        for l in self.layers:\n",
    "            if hasattr(l, 'weight') and hasattr(l, 'bias'):\n",
    "                ps.append(l.weight)\n",
    "                ps.append(l.bias)\n",
    "        return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367c79f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.733957Z",
     "iopub.status.busy": "2022-05-02T01:25:18.733114Z",
     "iopub.status.idle": "2022-05-02T01:25:18.735146Z",
     "shell.execute_reply": "2022-05-02T01:25:18.735851Z"
    },
    "id": "2gxTApUYTsqD"
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d643018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.744483Z",
     "iopub.status.busy": "2022-05-02T01:25:18.743617Z",
     "iopub.status.idle": "2022-05-02T01:25:18.745891Z",
     "shell.execute_reply": "2022-05-02T01:25:18.746578Z"
    },
    "id": "xqVTz9MkTsqD"
   },
   "outputs": [],
   "source": [
    "class CustomSequential(nn.Module):\n",
    "    def __init__(self, encoder_layers) -> None:\n",
    "        super(CustomSequential, self).__init__()\n",
    "        self.encoder_layers = encoder_layers\n",
    "        modules = []\n",
    "        for i in range(len(encoder_layers)):\n",
    "            modules.append(encoder_layers[i].sublayer[0])\n",
    "            modules.append(encoder_layers[i].feed_forward)\n",
    "            modules.append(encoder_layers[i].sublayer[1])\n",
    "        self.custom_sequential = nn.Sequential(*modules)\n",
    "\n",
    "class Encoder_MNet_As_Is(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N, skipping_layer_idx, use_layers_M_inference=True):\n",
    "        super(Encoder_MNet_As_Is, self).__init__()\n",
    "        self.use_layers_M_inference = use_layers_M_inference\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "        self.layer_idx_M1 = skipping_layer_idx\n",
    "        self.mnet1 = MNet_Transformer_As_Is(CustomSequential(self.layers[skipping_layer_idx[0]:]).custom_sequential, (BATCH_SIZE, MODEL_DIM))\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if idx == self.layer_idx_M1[0]:\n",
    "                if self.use_layers_M_inference:\n",
    "                    x = self.mnet1(x, mask)\n",
    "                # else continue to next layer\n",
    "            elif idx in self.layer_idx_M1:\n",
    "\n",
    "                continue # already covered by the MNet\n",
    "            else:\n",
    "                x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "\n",
    "class Encoder_MNet_Compute_M(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N, skipping_layer_idx, use_layers_M_inference=True):\n",
    "        super(Encoder_MNet_Compute_M, self).__init__()\n",
    "        self.use_layers_M_inference = use_layers_M_inference\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "        self.layer_idx_M1 = skipping_layer_idx\n",
    "        self.mnet1 = MNet_Transformer_Compute_M(CustomSequential(self.layers[skipping_layer_idx[0]:]).custom_sequential, (BATCH_SIZE, MODEL_DIM))\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if idx == self.layer_idx_M1[0]:\n",
    "                if self.use_layers_M_inference:\n",
    "                    x = self.mnet1(x, mask)\n",
    "                # else continue to next layer\n",
    "            elif idx in self.layer_idx_M1:\n",
    "\n",
    "                continue # already covered by the MNet\n",
    "            else:\n",
    "                x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Encoder_Base(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N, use_layers_M_inference=True):\n",
    "        super(Encoder_Base, self).__init__()\n",
    "        self.use_layers_M_inference = use_layers_M_inference\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "        self.layer_idx_M1 = [3,4,5]\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if idx in self.layer_idx_M1 and not self.use_layers_M_inference:\n",
    "                continue # skip these layers for logic segmentation experiment\n",
    "            else:\n",
    "                x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb56a59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.756189Z",
     "iopub.status.busy": "2022-05-02T01:25:18.755315Z",
     "iopub.status.idle": "2022-05-02T01:25:18.757924Z",
     "shell.execute_reply": "2022-05-02T01:25:18.758590Z"
    },
    "id": "3jKa_prZTsqE"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        try:\n",
    "            return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "        except RuntimeError:\n",
    "            return self.a_2.cuda('cuda:1') * (x - mean) / (std + self.eps) + self.b_2.cuda('cuda:1')\n",
    "        \n",
    "class LayerNorm_CUDA(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm_CUDA, self).__init__()\n",
    "\n",
    "        self.a_2 = nn.Parameter(torch.ones(features)).to('cuda:1')\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features)).to('cuda:1')\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17853e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_func_M = nn.Softmax(dim=-1)\n",
    "layerwisenorm_M = LayerNorm_CUDA(m_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f34584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.767044Z",
     "iopub.status.busy": "2022-05-02T01:25:18.766174Z",
     "iopub.status.idle": "2022-05-02T01:25:18.768689Z",
     "shell.execute_reply": "2022-05-02T01:25:18.769383Z"
    },
    "id": "U1P7zI0eTsqE"
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_mat = None\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "\n",
    "        imd = sublayer(self.norm(x))\n",
    "        if type(imd) is tuple:\n",
    "            self.attn_mat = imd[-1] # grab second output from MHAttn\n",
    "        else:\n",
    "            imd = [imd, -1]\n",
    "        return x + self.dropout(imd[0]) # regular opeartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db97336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.779893Z",
     "iopub.status.busy": "2022-05-02T01:25:18.778804Z",
     "iopub.status.idle": "2022-05-02T01:25:18.780994Z",
     "shell.execute_reply": "2022-05-02T01:25:18.781710Z"
    },
    "id": "qYkUFr6GTsqE"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe403d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.790602Z",
     "iopub.status.busy": "2022-05-02T01:25:18.789754Z",
     "iopub.status.idle": "2022-05-02T01:25:18.791985Z",
     "shell.execute_reply": "2022-05-02T01:25:18.792756Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1df6b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.803099Z",
     "iopub.status.busy": "2022-05-02T01:25:18.796132Z",
     "iopub.status.idle": "2022-05-02T01:25:18.806479Z",
     "shell.execute_reply": "2022-05-02T01:25:18.805667Z"
    },
    "id": "M2hA1xFQTsqF"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1b467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.813309Z",
     "iopub.status.busy": "2022-05-02T01:25:18.812387Z",
     "iopub.status.idle": "2022-05-02T01:25:18.815171Z",
     "shell.execute_reply": "2022-05-02T01:25:18.814519Z"
    },
    "id": "QN98O2l3TsqF"
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92d7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:19.251181Z",
     "iopub.status.busy": "2022-05-02T01:25:19.250905Z",
     "iopub.status.idle": "2022-05-02T01:25:19.253190Z",
     "shell.execute_reply": "2022-05-02T01:25:19.253437Z"
    },
    "id": "qsoVxS5yTsqG",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7901bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:19.265961Z",
     "iopub.status.busy": "2022-05-02T01:25:19.265360Z",
     "iopub.status.idle": "2022-05-02T01:25:19.267873Z",
     "shell.execute_reply": "2022-05-02T01:25:19.267306Z"
    },
    "id": "D2LBMKCQTsqH"
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None # this is the output of softmax (before multiplied by V) ------------------ NOTE\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x), self.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf547802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:19.274902Z",
     "iopub.status.busy": "2022-05-02T01:25:19.274161Z",
     "iopub.status.idle": "2022-05-02T01:25:19.277708Z",
     "shell.execute_reply": "2022-05-02T01:25:19.278473Z"
    },
    "id": "6HHCemCxTsqH"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            return self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "        except RuntimeError:\n",
    "            return (self.w_2.cuda('cuda:1'))(self.dropout(self.w_1.cuda('cuda:1')(x).relu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851b029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:19.286884Z",
     "iopub.status.busy": "2022-05-02T01:25:19.285947Z",
     "iopub.status.idle": "2022-05-02T01:25:19.288160Z",
     "shell.execute_reply": "2022-05-02T01:25:19.288878Z"
    },
    "id": "pyrChq9qTsqH"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfacc553",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:19.300508Z",
     "iopub.status.busy": "2022-05-02T01:25:19.299554Z",
     "iopub.status.idle": "2022-05-02T01:25:19.301927Z",
     "shell.execute_reply": "2022-05-02T01:25:19.302701Z"
    },
    "id": "zaHGD4yJTsqH"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e5768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:19.633281Z",
     "iopub.status.busy": "2022-05-02T01:25:19.632424Z",
     "iopub.status.idle": "2022-05-02T01:25:19.634909Z",
     "shell.execute_reply": "2022-05-02T01:25:19.634216Z"
    },
    "id": "mPe1ES0UTsqI"
   },
   "outputs": [],
   "source": [
    "def make_model_As_Is(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1, use_layers_M_inference = True, skipping_layer_idx = [3,4,5]\n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder_MNet_As_Is(EncoderLayer(d_model, c(attn), c(ff), dropout), N, skipping_layer_idx, use_layers_M_inference=use_layers_M_inference),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "def make_model_Compute_M(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1, use_layers_M_inference = True, skipping_layer_idx = [3,4,5]\n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder_MNet_Compute_M(EncoderLayer(d_model, c(attn), c(ff), dropout), N, skipping_layer_idx, use_layers_M_inference=use_layers_M_inference),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "def make_model_Base(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1, use_layers_M_inference = True,\n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder_Base(EncoderLayer(d_model, c(attn), c(ff), dropout), N, use_layers_M_inference=use_layers_M_inference),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee20bc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:20.889839Z",
     "iopub.status.busy": "2022-05-02T01:25:20.889563Z",
     "iopub.status.idle": "2022-05-02T01:25:20.890646Z",
     "shell.execute_reply": "2022-05-02T01:25:20.892925Z"
    }
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n",
    "\n",
    "    def __init__(self, src, tgt=None, pad=2):  # 2 = <blank>\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]\n",
    "            self.tgt_y = tgt[:, 1:]\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32298c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:20.895558Z",
     "iopub.status.busy": "2022-05-02T01:25:20.895196Z",
     "iopub.status.idle": "2022-05-02T01:25:20.896378Z",
     "shell.execute_reply": "2022-05-02T01:25:20.896666Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "\n",
    "    step: int = 0  # Steps in the current epoch\n",
    "    accum_step: int = 0  # Number of gradient accumulation steps\n",
    "    samples: int = 0  # total # of examples used\n",
    "    tokens: int = 0  # total # of tokens processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa4e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "):\n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(\n",
    "            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n",
    "        )\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "        \n",
    "        # loss_node = loss_node / accum_iter\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "            training_loss.append(loss)\n",
    "        else:\n",
    "            val_loss.append(loss)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        del loss\n",
    "        del loss_node\n",
    "        \n",
    "    return total_loss / total_tokens, train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013f248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:20.901877Z",
     "iopub.status.busy": "2022-05-02T01:25:20.897925Z",
     "iopub.status.idle": "2022-05-02T01:25:20.903288Z",
     "shell.execute_reply": "2022-05-02T01:25:20.902928Z"
    },
    "id": "2HAZD3hiTsqJ"
   },
   "outputs": [],
   "source": [
    "def run_epoch_M(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "    use_M=True\n",
    "):\n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(\n",
    "            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n",
    "        )\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "        training_loss.append(loss)\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()\n",
    "\n",
    "\n",
    "            # update M's ----------------------------------------------\n",
    "            if use_M:\n",
    "                model.encoder.mnet1.backwardHidden()\n",
    "\n",
    "\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        del loss\n",
    "        del loss_node\n",
    "        \n",
    "    return total_loss / total_tokens, train_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dccf0a51",
   "metadata": {},
   "source": [
    "MNET ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ded57a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:20.906007Z",
     "iopub.status.busy": "2022-05-02T01:25:20.904743Z",
     "iopub.status.idle": "2022-05-02T01:25:20.907701Z",
     "shell.execute_reply": "2022-05-02T01:25:20.907236Z"
    },
    "id": "zUz3PdAnVg4o"
   },
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d96f50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:22.858827Z",
     "iopub.status.busy": "2022-05-02T01:25:22.858459Z",
     "iopub.status.idle": "2022-05-02T01:25:22.860907Z",
     "shell.execute_reply": "2022-05-02T01:25:22.861184Z"
    },
    "id": "shU2GyiETsqK",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88cbe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:23.462842Z",
     "iopub.status.busy": "2022-05-02T01:25:23.461999Z",
     "iopub.status.idle": "2022-05-02T01:25:23.464005Z",
     "shell.execute_reply": "2022-05-02T01:25:23.464702Z"
    },
    "id": "3J8EJm87TsqK"
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        return sloss.data * norm, sloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426fe69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:23.470843Z",
     "iopub.status.busy": "2022-05-02T01:25:23.469910Z",
     "iopub.status.idle": "2022-05-02T01:25:23.471795Z",
     "shell.execute_reply": "2022-05-02T01:25:23.472524Z"
    },
    "id": "N2UOpnT3bIyU",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len - 1):\n",
    "        out = model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a83e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:23.491667Z",
     "iopub.status.busy": "2022-05-02T01:25:23.490728Z",
     "iopub.status.idle": "2022-05-02T01:25:23.492689Z",
     "shell.execute_reply": "2022-05-02T01:25:23.493407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load spacy tokenizer models, download them if they haven't been\n",
    "# downloaded already\n",
    "\n",
    "\n",
    "def load_tokenizers():\n",
    "\n",
    "    try:\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download de_core_news_sm\")\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    return spacy_de, spacy_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092be4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:23.500603Z",
     "iopub.status.busy": "2022-05-02T01:25:23.499698Z",
     "iopub.status.idle": "2022-05-02T01:25:23.502449Z",
     "shell.execute_reply": "2022-05-02T01:25:23.501724Z"
    },
    "id": "t4BszXXJTsqL",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer):\n",
    "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer, index):\n",
    "    for from_to_tuple in data_iter:\n",
    "        yield tokenizer(from_to_tuple[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64878fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:23.516475Z",
     "iopub.status.busy": "2022-05-02T01:25:23.515578Z",
     "iopub.status.idle": "2022-05-02T01:25:25.130885Z",
     "shell.execute_reply": "2022-05-02T01:25:25.130120Z"
    },
    "id": "jU3kVlV5okC-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(spacy_de, spacy_en):\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    print(\"Building German Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_src = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_de, index=0),\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    print(\"Building English Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_tgt = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_en, index=1),\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    vocab_src.set_default_index(vocab_src[\"<unk>\"])\n",
    "    vocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\n",
    "\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "\n",
    "def load_vocab(spacy_de, spacy_en):\n",
    "    if not exists(\"vocab.pt\") or True:\n",
    "        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\n",
    "        torch.save((vocab_src, vocab_tgt), \"vocab.pt\")\n",
    "    else:\n",
    "        vocab_src, vocab_tgt = torch.load(\"vocab.pt\")\n",
    "    print(\"Finished.\\nVocabulary sizes:\")\n",
    "    print(len(vocab_src))\n",
    "    print(len(vocab_tgt))\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "\n",
    "if is_interactive_notebook():\n",
    "    # global variables used later in the script\n",
    "    spacy_de1, spacy_en1 = show_example(load_tokenizers)\n",
    "    vocab_src1, vocab_tgt1 = show_example(load_vocab, args=[spacy_de1, spacy_en1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33350048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:25.144310Z",
     "iopub.status.busy": "2022-05-02T01:25:25.143931Z",
     "iopub.status.idle": "2022-05-02T01:25:25.146354Z",
     "shell.execute_reply": "2022-05-02T01:25:25.146629Z"
    },
    "id": "wGsIHFgOokC_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_batch(\n",
    "    batch,\n",
    "    src_pipeline,\n",
    "    tgt_pipeline,\n",
    "    src_vocab,\n",
    "    tgt_vocab,\n",
    "    device,\n",
    "    max_padding=128,\n",
    "    pad_id=2,\n",
    "    language_pairing = None # only need for iwslt\n",
    "):\n",
    "    bs_id = torch.tensor([0], device=device)  # <s> token id\n",
    "    eos_id = torch.tensor([1], device=device)  # </s> token id\n",
    "    src_list, tgt_list = [], []\n",
    "    try:\n",
    "        _src, _tgt = batch[0]\n",
    "    except ValueError:\n",
    "        batch = [(batch[i]['translation'][language_pairing[0]], batch[i]['translation'][language_pairing[1]]) for i in range(len(batch))]\n",
    "\n",
    "    for (_src, _tgt) in batch:\n",
    "        processed_src = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    src_vocab(src_pipeline(_src)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        processed_tgt = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    tgt_vocab(tgt_pipeline(_tgt)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        src_list.append(\n",
    "            # warning - overwrites values for negative values of padding - len\n",
    "            pad(\n",
    "                processed_src,\n",
    "                (\n",
    "                    0,\n",
    "                    max_padding - len(processed_src),\n",
    "                ),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "        tgt_list.append(\n",
    "            pad(\n",
    "                processed_tgt,\n",
    "                (0, max_padding - len(processed_tgt)),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    src = torch.stack(src_list)\n",
    "    tgt = torch.stack(tgt_list)\n",
    "    return (src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d94df2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:25.158739Z",
     "iopub.status.busy": "2022-05-02T01:25:25.158133Z",
     "iopub.status.idle": "2022-05-02T01:25:25.160905Z",
     "shell.execute_reply": "2022-05-02T01:25:25.160371Z"
    },
    "id": "ka2Ce_WIokC_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset # for huggingface datasets\n",
    "\n",
    "def create_dataloaders(\n",
    "    device,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    batch_size=12000,\n",
    "    max_padding=128,\n",
    "    is_distributed=False,\n",
    "    language_pair = ('de', 'en'),\n",
    "    dataset_func = datasets.IWSLT2017\n",
    "):\n",
    "    # def create_dataloaders(batch_size=12000):\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return collate_batch(\n",
    "            batch,\n",
    "            tokenize_de,\n",
    "            tokenize_en,\n",
    "            vocab_src,\n",
    "            vocab_tgt,\n",
    "            device,\n",
    "            max_padding=max_padding,\n",
    "            pad_id=vocab_src.get_stoi()[\"<blank>\"],\n",
    "            language_pairing=language_pair\n",
    "        )\n",
    "    dataset_func = datasets.IWSLT2017\n",
    "    if 'iwslt' in str(dataset_func).lower():\n",
    "        dataset = load_dataset(\"iwslt2017\", f'iwslt2017-{language_pair[0]}-{language_pair[1]}')\n",
    "        train_iter, valid_iter, test_iter = dataset['train'], dataset['validation'], dataset['test']\n",
    "\n",
    "    else: # just multi30k \n",
    "        train_iter, valid_iter, test_iter = dataset_func(\n",
    "            language_pair=language_pair\n",
    "        )\n",
    "    try:\n",
    "        # raise Exception\n",
    "        train_iter_map = to_map_style_dataset(\n",
    "            train_iter\n",
    "        )  # DistributedSampler needs a dataset len()\n",
    "        train_sampler = (\n",
    "            DistributedSampler(train_iter_map) if is_distributed else None\n",
    "        )\n",
    "        valid_iter_map = to_map_style_dataset(valid_iter)\n",
    "        valid_sampler = (\n",
    "            DistributedSampler(valid_iter_map) if is_distributed else None\n",
    "        )\n",
    "    except:\n",
    "        train_iter_map = train_iter\n",
    "        valid_iter_map = valid_iter\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        sampler=None,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        sampler=None,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "\n",
    "def create_dataloader_test(\n",
    "    device,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    batch_size=12000,\n",
    "    max_padding=128,\n",
    "    is_distributed=False,\n",
    "    language_pair = ('de', 'en'),\n",
    "    dataset_func = datasets.IWSLT2017\n",
    "):\n",
    "    # def create_dataloaders(batch_size=12000)        \n",
    "\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return collate_batch(\n",
    "            batch,\n",
    "            tokenize_de,\n",
    "            tokenize_en,\n",
    "            vocab_src,\n",
    "            vocab_tgt,\n",
    "            device,\n",
    "            max_padding=max_padding,\n",
    "            pad_id=vocab_src.get_stoi()[\"<blank>\"],\n",
    "            language_pairing=language_pair\n",
    "        )\n",
    "    dataset_func = datasets.IWSLT2017\n",
    "    if 'iwslt' in str(dataset_func).lower():\n",
    "        dataset = load_dataset(\"iwslt2017\", f'iwslt2017-{language_pair[0]}-{language_pair[1]}')\n",
    "        train_iter, valid_iter, test_iter = dataset['train'], dataset['validation'], dataset['test']\n",
    "    else:\n",
    "        train_iter, valid_iter, test_iter = dataset_func(\n",
    "            language_pair=language_pair\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        test_iter_map = to_map_style_dataset(\n",
    "            test_iter\n",
    "        )  # DistributedSampler needs a dataset len()\n",
    "        test_sampler = (\n",
    "            DistributedSampler(test_iter_map) if is_distributed else None\n",
    "        )\n",
    "    except:\n",
    "        test_iter_map = test_iter\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        sampler=None,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b763750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:25.181295Z",
     "iopub.status.busy": "2022-05-02T01:25:25.180613Z",
     "iopub.status.idle": "2022-05-02T01:25:25.183519Z",
     "shell.execute_reply": "2022-05-02T01:25:25.183983Z"
    }
   },
   "outputs": [],
   "source": [
    "training_loss = []\n",
    "val_loss = []\n",
    "\n",
    "def train_worker(\n",
    "    gpu,\n",
    "    ngpus_per_node,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    config,\n",
    "    is_distributed=False,\n",
    "    model_name = 'base', # base, as_is, compute_m\n",
    "    dataset_func = datasets.Multi30k,\n",
    "    language_pair=('de','en'),\n",
    "    iter_num=-1,\n",
    "    timestamp=time.time(),\n",
    "    N = 6,\n",
    "    skipping_layer_idx = [3,4,5]\n",
    "\n",
    "):\n",
    "    time_start = time.time()\n",
    "    print(f\"Train worker process using GPU: {gpu} for training\", flush=True)\n",
    "    torch.cuda.set_device(gpu)\n",
    "\n",
    "    pad_idx = vocab_tgt[\"<blank>\"]\n",
    "    d_model = 512\n",
    "\n",
    "    if model_name == 'base':\n",
    "        model = make_model_Base(len(vocab_src), len(vocab_tgt), N=N)\n",
    "    elif model_name == 'as_is':\n",
    "        model = make_model_As_Is(len(vocab_src), len(vocab_tgt), N=N, skipping_layer_idx = skipping_layer_idx)\n",
    "    elif model_name == 'compute_m':\n",
    "        model = make_model_Compute_M(len(vocab_src), len(vocab_tgt), N=N, skipping_layer_idx = skipping_layer_idx)\n",
    "    else:\n",
    "        raise NotImplementedError('Model name not implemented: please use one of the following: [base, as_is, compute_m]')\n",
    "\n",
    "    \n",
    "    model.cuda(gpu)\n",
    "    module = model\n",
    "    is_main_process = True\n",
    "    if is_distributed:\n",
    "        dist.init_process_group(\n",
    "            \"nccl\", init_method=\"env://\", rank=gpu, world_size=ngpus_per_node\n",
    "        )\n",
    "        model = DDP(model, device_ids=[gpu])\n",
    "        module = model.module\n",
    "        is_main_process = gpu == 0\n",
    "\n",
    "    criterion = LabelSmoothing(\n",
    "        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n",
    "    )\n",
    "    criterion.cuda(gpu)\n",
    "\n",
    "    train_dataloader, valid_dataloader = create_dataloaders(\n",
    "        gpu,\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        batch_size=config[\"batch_size\"] // ngpus_per_node,\n",
    "        max_padding=config[\"max_padding\"],\n",
    "        is_distributed=is_distributed,\n",
    "        language_pair=language_pair,\n",
    "        dataset_func=dataset_func\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, d_model, factor=1, warmup=config[\"warmup\"]\n",
    "        ),\n",
    "    )\n",
    "    train_state = TrainState()\n",
    "    best_valid_loss = -999999\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        if is_distributed:\n",
    "            train_dataloader.sampler.set_epoch(epoch)\n",
    "            valid_dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        model.train()\n",
    "        try:\n",
    "            model.encoder.mnet1.use_M = True \n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n",
    "        s = time.time()\n",
    "        if model_name == 'as_is' or model_name == 'compute_m':\n",
    "            _, train_state = run_epoch_M(\n",
    "                (Batch(b[0], b[1], pad_idx) for b in train_dataloader),\n",
    "                model,\n",
    "                SimpleLossCompute(module.generator, criterion),\n",
    "                optimizer,\n",
    "                lr_scheduler,\n",
    "                mode=\"train+log\",\n",
    "                accum_iter=config[\"accum_iter\"],\n",
    "                train_state=train_state,\n",
    "                use_M = True,\n",
    "            )\n",
    "        else:\n",
    "            _, train_state = run_epoch( # for base model\n",
    "                (Batch(b[0], b[1], pad_idx) for b in train_dataloader),\n",
    "                model,\n",
    "                SimpleLossCompute(module.generator, criterion),\n",
    "                optimizer,\n",
    "                lr_scheduler,\n",
    "                mode=\"train+log\",\n",
    "                accum_iter=config[\"accum_iter\"],\n",
    "                train_state=train_state,\n",
    "            )\n",
    "        print(f\"Epoch training time: {time.time() - s}\")\n",
    "\n",
    "        GPUtil.showUtilization()\n",
    "        if is_main_process:\n",
    "            file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n",
    "            torch.save(module.state_dict(), file_path)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\n",
    "        model.eval()\n",
    "        sloss = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )\n",
    "        if best_valid_loss < 0:\n",
    "            best_valid_loss = sloss[0]\n",
    "        elif best_valid_loss > sloss[0]:\n",
    "            best_valid_loss = sloss[0]\n",
    "            print('best performance saved')\n",
    "            file_path = f\"%sfinal_best_val_loss_{model_name}_{N}_{timestamp}_trial_{iter_num}.pt\" % config[\"file_prefix\"]\n",
    "            torch.save(module.state_dict(), file_path)\n",
    "        print(sloss)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    if is_main_process:\n",
    "        otherinfo = {\n",
    "            'experiment name' : 'E6D6 - last 3 layers of E are abstracted by M (if not base)',\n",
    "            'total wallclock training time' : time.time()-time_start,\n",
    "            'config_info' : config\n",
    "        }\n",
    "        file_path = f\"%sfinal_{model_name}_{N}_{timestamp}_trial_{iter_num}.pt\" % config[\"file_prefix\"]\n",
    "        torch.save(training_loss, f'saved_{model_name}_{N}_{timestamp}_trial_{iter_num}_train_losses.pt')\n",
    "        torch.save(val_loss, f'saved_{model_name}_{N}_{timestamp}_trial_{iter_num}_val_losses.pt')\n",
    "        torch.save(otherinfo, f'saved_{model_name}_{N}_{timestamp}_trial_{iter_num}_otherinfo.pt')\n",
    "        torch.save(module.state_dict(), file_path)\n",
    "        file_path1 = f\"%sfinal.pt\" % config[\"file_prefix\"]\n",
    "        torch.save(module.state_dict(), file_path1)\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a913b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:25.195835Z",
     "iopub.status.busy": "2022-05-02T01:25:25.194849Z",
     "iopub.status.idle": "2022-05-02T01:25:28.698817Z",
     "shell.execute_reply": "2022-05-02T01:25:28.699555Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "IS_CUDA = True\n",
    "H = 8\n",
    "BATCH_SIZE = 32\n",
    "MODEL_DIM = 512\n",
    "def load_trained_model(model_name, iter_num, timestamp, N = 6, skipping_layer_idx = [3,4,5], dataset_func = datasets.IWSLT2017):\n",
    "    config = {\n",
    "        \"batch_size\": 32,\n",
    "        \"distributed\": False,\n",
    "        \"num_epochs\": 14,\n",
    "        \"accum_iter\": 10,\n",
    "        \"base_lr\": 1.0,\n",
    "        \"max_padding\": 72,\n",
    "        \"warmup\": 3000,\n",
    "        \"file_prefix\": \"multi30k_model_\",\n",
    "    }\n",
    "    model_path = \"multi30k_model_final.pt\"\n",
    "    if not exists(model_path) or True: # default to always train\n",
    "        model_fp = train_worker(\n",
    "        1, 1, vocab_src, vocab_tgt, spacy_de, spacy_en, config, False, model_name=model_name, iter_num=iter_num, timestamp=timestamp,\n",
    "        N = N, skipping_layer_idx = skipping_layer_idx \n",
    "    )\n",
    "    print(f\"Done training {model_name}, config_info={config}\")\n",
    "    return model_fp\n",
    "\n",
    "s = time.time()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54a74050",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d77e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 5 training runs on 1 dataset (for now) and one language pairing (for now de->en), be sure to reference config in load_trained_model to adjust epochs\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "GPUtil.showUtilization()\n",
    "\n",
    "num_runs = 5\n",
    "datasets_list = [datasets.IWSLT2017]\n",
    "language_pairings = [('de', 'en')]\n",
    "model_names = ['base','as_is', 'compute_m']\n",
    "\n",
    "for j in range(len(datasets_list)):\n",
    "    for lp in language_pairings:\n",
    "        if lp == ('de', 'en'):\n",
    "            global_lp_flag = lp\n",
    "            spacy_de, spacy_en = spacy_de1, spacy_en1\n",
    "            vocab_src, vocab_tgt = vocab_src1, vocab_tgt1\n",
    "        else:\n",
    "            global_lp_flag = lp\n",
    "            spacy_de, spacy_en = spacy_de2, spacy_en2\n",
    "            vocab_src, vocab_tgt = vocab_src2, vocab_tgt2\n",
    "        for mn in model_names:\n",
    "            print(\"Model Name\", mn, 'trails starts ============================')\n",
    "            timestamp = 5032023\n",
    "            for i in range(num_runs):\n",
    "                print('running', mn, 'N', 6)\n",
    "                training_loss = []\n",
    "                val_loss = []\n",
    "                model_fp = load_trained_model(model_name=mn, iter_num=i, timestamp=timestamp, N = 6, skipping_layer_idx = [3,4,5]) # execute training and save model\n",
    "                \n",
    "                print(f'Trial: {i}. {mn} model trained successfully see file {model_fp} for model weights')\n",
    "            print(\"Model Name\", mn, 'trails ends ============================ \\n\\n')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13076241",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e838703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "refs = []\n",
    "preds = []\n",
    "src = []\n",
    "\n",
    "\n",
    "def check_outputs(\n",
    "    valid_dataloader,\n",
    "    model,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    n_examples=15,\n",
    "    pad_idx=2,\n",
    "    eos_string=\"</s>\",\n",
    "):\n",
    "    results = [()] * n_examples\n",
    "\n",
    "    for idx in tqdm(range(n_examples)):\n",
    "        b = next(iter(valid_dataloader))\n",
    "        rb = Batch(b[0], b[1], pad_idx)\n",
    "        rb.src = rb.src.to('cuda:1')\n",
    "        rb.src_mask = rb.src_mask.to('cuda:1')\n",
    "\n",
    "        src_tokens = [\n",
    "            vocab_src.get_itos()[x] for x in rb.src[0] if x != pad_idx\n",
    "        ]\n",
    "        \n",
    "        # save the src for comet\n",
    "        src.append(\" \".join(src_tokens).replace(\"\\n\", \"\"))\n",
    "        \n",
    "        tgt_tokens = [\n",
    "            vocab_tgt.get_itos()[x] for x in rb.tgt[0] if x != pad_idx\n",
    "        ]\n",
    "\n",
    "\n",
    "        refs.append(\" \".join(tgt_tokens).replace(\"\\n\", \"\"))\n",
    "\n",
    "        model_out = greedy_decode(model, rb.src, rb.src_mask, 72, 0)[0]\n",
    "        model_txt = (\n",
    "            \" \".join(\n",
    "                [vocab_tgt.get_itos()[x] for x in model_out if x != pad_idx]\n",
    "            ).split(eos_string, 1)[0]\n",
    "            + eos_string\n",
    "        )\n",
    "        preds.append(model_txt.replace(\"\\n\", \"\"))\n",
    "        results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89a5935c",
   "metadata": {},
   "source": [
    "Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba09364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "do_test = True\n",
    "path_root = './'\n",
    "path_root_to_save = './'\n",
    "N_sizes = [6]\n",
    "\n",
    "if do_test:\n",
    "\n",
    "    model_names = ['base','as_is', 'compute_m']\n",
    "    language_pairings = [('de', 'en')]\n",
    "    num_trials = 5\n",
    "\n",
    "\n",
    "    for mn in model_names:\n",
    "        for lp in language_pairings:\n",
    "\n",
    "            # set global vocab vars\n",
    "            if lp == ('de', 'en'):\n",
    "                spacy_de, spacy_en = spacy_de1, spacy_en1\n",
    "                vocab_src, vocab_tgt = vocab_src1, vocab_tgt1\n",
    "            else:\n",
    "                spacy_de, spacy_en = spacy_de2, spacy_en2\n",
    "                vocab_src, vocab_tgt = vocab_src2, vocab_tgt2\n",
    "\n",
    "            for i in range(num_trials):\n",
    "                for N in N_sizes:\n",
    "                    # find path\n",
    "                    if mn != 'base' and N == 1:\n",
    "                        continue\n",
    "                    if N == 3:\n",
    "                        skipping_layer_idx = [2]\n",
    "                    else:\n",
    "                        skipping_layer_idx = [3,4,5]\n",
    "                        \n",
    "                    paths = glob.glob(f'{path_root}/*')\n",
    "                    p_ct = 0\n",
    "                    for p in paths:\n",
    "                        if f'{mn}_{N}' in p and f'best_val_loss' in p and f'trial_{i}.pt' in p:\n",
    "                            path = p\n",
    "                            p_ct += 1\n",
    "                            print(path)\n",
    "                    \n",
    "                    assert p_ct == 1\n",
    "                    \n",
    "                    # reset lists\n",
    "                    refs = []\n",
    "                    preds = []\n",
    "                    src = []\n",
    "\n",
    "                    # init correct model\n",
    "                    if mn == 'base':\n",
    "                        model = make_model_Base(len(vocab_src), len(vocab_tgt), N=N)\n",
    "                    elif mn == 'as_is':\n",
    "                        model = make_model_As_Is(len(vocab_src), len(vocab_tgt), N=N, skipping_layer_idx = skipping_layer_idx)\n",
    "                    elif mn == 'compute_m':\n",
    "                        model = make_model_Compute_M(len(vocab_src), len(vocab_tgt), N=N, skipping_layer_idx = skipping_layer_idx)\n",
    "\n",
    "                    # load model weights\n",
    "                    print(\"Loading model weights from \", path)\n",
    "                    model.load_state_dict(torch.load(path))\n",
    "                    model = model.to('cuda:1')\n",
    "                    model.eval()\n",
    "\n",
    "                    test_dataloader = create_dataloader_test(\n",
    "                            torch.device(\"cuda:1\"),\n",
    "                            vocab_src,\n",
    "                            vocab_tgt,\n",
    "                            spacy_de,\n",
    "                            spacy_en,\n",
    "                            batch_size=1,\n",
    "                            is_distributed=False,\n",
    "                        )\n",
    "                    print(len(test_dataloader))\n",
    "                    \n",
    "                    print(\"Checking Model Outputs:\")\n",
    "                    example_data_M = check_outputs(\n",
    "                        test_dataloader, model, vocab_src, vocab_tgt, n_examples= len(test_dataloader) # limit this n_examples for faster computation\n",
    "                    )\n",
    "\n",
    "\n",
    "                    # create dir if not exists\n",
    "                    if not os.path.exists(path_root_to_save):\n",
    "                        os.makedirs(path_root_to_save)\n",
    "                    lp_str = f'{lp[0]}->{lp[1]}'\n",
    "                    # save the preds, refs, src for this specific model \n",
    "                    fo = open(f'{path_root_to_save}/model={mn}_N={N}_lang_pair={lp_str}_trial={i}_preds.txt', 'w')\n",
    "                    for sentence in preds:\n",
    "                        if len(sentence) == 1:\n",
    "                            sentence = sentence[0]\n",
    "                        sentence = (sentence.replace('<s>', '').replace('</s>', '')).strip()\n",
    "                        fo.write(\"\".join(sentence) + '\\n')\n",
    "                        # print(\"\".join(sentence))\n",
    "                    fo.close()\n",
    "\n",
    "                    fo = open(f'{path_root_to_save}/model={mn}_N={N}_lang_pair={lp_str}_trial={i}_refs.txt', 'w')\n",
    "                    for sentence in refs:\n",
    "                        if len(sentence) == 1:\n",
    "                            sentence = sentence[0]\n",
    "                        sentence = (sentence.replace('<s>', '').replace('</s>', '')).strip()\n",
    "                        fo.write(\"\".join(sentence) + '\\n')\n",
    "                        # print(\"\".join(sentence))\n",
    "                    fo.close()\n",
    "\n",
    "                    fo = open(f'{path_root_to_save}/model={mn}_N={N}_lang_pair={lp_str}_trial={i}_src.txt', 'w')\n",
    "                    for sentence in src:\n",
    "                        if len(sentence) == 1:\n",
    "                            sentence = sentence[0]\n",
    "                        sentence = (sentence.replace('<s>', '').replace('</s>', '')).strip()\n",
    "                        fo.write(\"\".join(sentence) + '\\n')\n",
    "                        # print(\"\".join(sentence))\n",
    "                    fo.close()\n",
    "\n",
    "                    print('\\nDone writing preds, refs, and src files for this model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "835ca950",
   "metadata": {},
   "source": [
    "Compute Prediction Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torchmetrics\n",
    "except:\n",
    "    !pip install torchmetrics\n",
    "\n",
    "try:\n",
    "    import evaluate\n",
    "except:\n",
    "    !pip install evaluate\n",
    "\n",
    "from torchmetrics import BLEUScore, SacreBLEUScore\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8098a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(fn, data):\n",
    "    with open(fn) as file:\n",
    "        while line := file.readline():\n",
    "            data.append(line.rstrip())\n",
    "            # break\n",
    "    return data\n",
    "\n",
    "model_names = ['base', 'as_is', 'compute_m']\n",
    "language_pairings = [('de', 'en')]\n",
    "num_trials = 5\n",
    "N_lists = ['6']\n",
    "path_root = './'\n",
    "paths = glob.glob(f'{path_root}/*')\n",
    "\n",
    "df_dict = {}\n",
    "for mn in model_names:\n",
    "    for lp in language_pairings:\n",
    "        for N in N_lists:\n",
    "            if mn != 'base' and N =='1':\n",
    "                continue\n",
    "            lp_str = f'{lp[0]}->{lp[1]}'\n",
    "            df_dict[f'{mn}_{lp_str}_{N}'] = {\n",
    "                'BLEU' : (0,0), # format key as tuple (mean, std)\n",
    "                'SacreBLEU' : (0,0),\n",
    "                'METEOR' : (0,0),\n",
    "                'COMET' : (0,0)\n",
    "            }\n",
    "            metrics_dict = {\n",
    "                'bleu' : [],\n",
    "                'sacrebleu' : [],\n",
    "                'meteor' : [],\n",
    "                'comet' : []\n",
    "            }\n",
    "            for i in range(num_trials):\n",
    "                \n",
    "                p_ct = 0\n",
    "                for p in paths:\n",
    "                    if mn in p and f'trial={i}' in p and f'N={N}' in p:\n",
    "                        if 'preds' in p:\n",
    "                            preds_path = p\n",
    "                        elif 'refs' in p:\n",
    "                            refs_path = p\n",
    "                        elif 'src' in p:\n",
    "                            src_path = p\n",
    "                        p_ct += 1\n",
    "                        print(p)\n",
    "                assert p_ct == 3\n",
    "\n",
    "                # read data into lists\n",
    "                preds = []\n",
    "                refs = []\n",
    "                src = []\n",
    "                readfile(preds_path, preds)\n",
    "                readfile(refs_path, refs)\n",
    "                readfile(src_path, src)\n",
    "\n",
    "                # bleu\n",
    "                if len(refs[0]) > 1:\n",
    "                    print('Adding surrounding list for each target')\n",
    "                    refs_bleu = [[x] for x in refs]\n",
    "                else:\n",
    "                    refs_bleu = refs\n",
    "\n",
    "                bleu = BLEUScore()\n",
    "                torch_bleu = bleu(preds, refs_bleu)\n",
    "                metrics_dict['bleu'].append(torch_bleu)\n",
    "\n",
    "                # sacrebleu\n",
    "                if len(refs[0]) > 1:\n",
    "                    print('Adding surrounding list for each target')\n",
    "                    refs_sacrebleu = [[x] for x in refs]\n",
    "                else:\n",
    "                    refs_sacrebleu = refs\n",
    "\n",
    "                sacre_bleu = SacreBLEUScore(tokenize='none')\n",
    "                torch_sacrebleu = sacre_bleu(preds, refs_sacrebleu)\n",
    "                metrics_dict['sacrebleu'].append(torch_sacrebleu)\n",
    "\n",
    "                # meteor\n",
    "                meteor = evaluate.load('meteor')\n",
    "                results = meteor.compute(predictions=preds, references=refs)\n",
    "                huggingface_meteor = (round(results['meteor'], 2))\n",
    "                metrics_dict['meteor'].append(huggingface_meteor)\n",
    "\n",
    "            # compute mean and std over trials and put in dict\n",
    "            mean_bleu = np.mean(np.array(metrics_dict['bleu']))\n",
    "            std_bleu = np.std(np.array(metrics_dict['bleu']))\n",
    "\n",
    "            mean_sacrebleu = np.mean(np.array(metrics_dict['sacrebleu']))\n",
    "            std_sacrebleu = np.std(np.array(metrics_dict['sacrebleu']))\n",
    "\n",
    "            mean_meteor = np.mean(np.array(metrics_dict['meteor']))\n",
    "            std_meteor = np.std(np.array(metrics_dict['meteor']))\n",
    "\n",
    "            # mean_comet = np.mean(np.array(metrics_dict['comet']))\n",
    "            # std_comet = np.std(np.array(metrics_dict['comet']))\n",
    "\n",
    "\n",
    "            df_dict[f'{mn}_{lp_str}_{N}'] = {\n",
    "                'BLEU' : (mean_bleu,std_bleu), # format key as tuple (mean, std)\n",
    "                'SacreBLEU' : (mean_sacrebleu,std_sacrebleu),\n",
    "                'METEOR' : (mean_meteor,std_meteor),\n",
    "                'COMET' : (0,0) # disabled for now\n",
    "            }\n",
    "\n",
    "results_df = pd.DataFrame(df_dict)\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
